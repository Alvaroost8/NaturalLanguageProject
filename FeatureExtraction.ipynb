{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import json\n",
    "import spacy_udpipe\n",
    "from langdetect import detect\n",
    "import string\n",
    "\n",
    "nlp_es = spacy.load('es_core_news_sm')\n",
    "nlp_ca = spacy.load('ca_core_news_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "f=open(\"training.json\")\n",
    "training_set=json.load(f)\n",
    "\n",
    "f2=open(\"test.json\")\n",
    "test_set=json.load(f2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_training=[]\n",
    "for i in range(len(training_set)):\n",
    "    l_training.append({\"text\":training_set[i][\"data\"][\"text\"], \"results\":training_set[i][\"predictions\"][0][\"result\"]})\n",
    "\n",
    "l_test=[]\n",
    "for i in range(len(test_set)):\n",
    "    l_test.append({\"text\":test_set[i][\"data\"][\"text\"], \"results\":test_set[i][\"predictions\"][0][\"result\"]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eliminate_patient_information(string):\n",
    "    index = string.find(\"informe\")\n",
    "    if index != -1:\n",
    "        return string[index:], index\n",
    "    else:\n",
    "        return string, 0\n",
    "\n",
    "def eliminate_final_information(string):\n",
    "    index = string.find(\"destinacio a l'alta\")\n",
    "    if index != -1:\n",
    "        return string[:index]\n",
    "    else:\n",
    "        return string\n",
    "\n",
    "def nhc_lopd(string):\n",
    "    nhc_indices = [i for i in range(len(string)) if string.startswith(\"nhc\", i)]\n",
    "    lopd_indices = [i+len(\"lopd\") for i in range(len(string)) if string.startswith(\"lopd\", i)]\n",
    "\n",
    "    results = []\n",
    "    for nhc_index in nhc_indices:\n",
    "        for lopd_index in lopd_indices:\n",
    "            if nhc_index < lopd_index:\n",
    "                length = lopd_index - nhc_index \n",
    "                results.append((nhc_index, length))\n",
    "                break  # break inner loop, move to next nhc_index\n",
    "\n",
    "    return results\n",
    "\n",
    "def correct_results(results, lenght, nhclopd):\n",
    "    for i in range(len(results)):\n",
    "        results[i][\"value\"][\"start\"]-=lenght\n",
    "        results[i][\"value\"][\"end\"]-=lenght\n",
    "        for j in nhclopd:\n",
    "            if(j[0]<results[i][\"value\"][\"start\"]):\n",
    "                results[i][\"value\"][\"start\"]-=j[1]\n",
    "                results[i][\"value\"][\"end\"]-=j[1]\n",
    "    return results\n",
    "\n",
    "def preprocessing(text_dict):\n",
    "    text=text_dict[\"text\"]\n",
    "    results=text_dict[\"results\"]\n",
    "    text, lenght=eliminate_patient_information(text)\n",
    "    text=eliminate_final_information(text)\n",
    "    nhclopd=nhc_lopd(text)\n",
    "    for i in (nhclopd):\n",
    "        index1=i[0]\n",
    "        index2=i[0]+i[1]\n",
    "        text=text[0:index1]+text[index2:]\n",
    "    results=correct_results(results, lenght, nhclopd)\n",
    "    text_dict[\"text\"]=text\n",
    "    text_dict[\"results\"]=results\n",
    "    return text_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_training_pro=[]\n",
    "for i in range(len(l_training)):\n",
    "    l_training_pro.append(preprocessing(l_training[i]))\n",
    "\n",
    "l_test_pro=[]\n",
    "for i in range(len(l_test)):\n",
    "    l_test_pro.append(preprocessing(l_test[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_neg=[]\n",
    "l_unc=[]\n",
    "l_nsco=[]\n",
    "l_usco=[]\n",
    "for dic in l_training_pro:\n",
    "    for dic2 in dic[\"results\"]:\n",
    "        if(dic2[\"value\"][\"labels\"][0]==\"NEG\"):\n",
    "            l_neg.append(dic[\"text\"][dic2[\"value\"][\"start\"]:dic2[\"value\"][\"end\"]])\n",
    "        \n",
    "        elif(dic2[\"value\"][\"labels\"][0]==\"UNC\"):\n",
    "            l_unc.append(dic[\"text\"][dic2[\"value\"][\"start\"]:dic2[\"value\"][\"end\"]])\n",
    "        \n",
    "        elif(dic2[\"value\"][\"labels\"][0]==\"NSCO\"):\n",
    "            l_nsco.append(dic[\"text\"][dic2[\"value\"][\"start\"]:dic2[\"value\"][\"end\"]])\n",
    "        \n",
    "        elif(dic2[\"value\"][\"labels\"][0]==\"USCO\"):\n",
    "            l_usco.append(dic[\"text\"][dic2[\"value\"][\"start\"]:dic2[\"value\"][\"end\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(l_neg)):\n",
    "    if(not l_neg[i][0].isalpha()):\n",
    "        l_neg[i]=l_neg[i][1:]\n",
    "    if(not l_neg[i][-1].isalpha()):\n",
    "        l_neg[i]=l_neg[i][:-1]\n",
    "\n",
    "for i in range(len(l_unc)):\n",
    "    if(not l_unc[i][0].isalpha()):\n",
    "        l_unc[i]=l_unc[i][1:]\n",
    "    if(not l_unc[i][-1].isalpha()):\n",
    "        l_unc[i]=l_unc[i][:-1]\n",
    "\n",
    "for i in range(len(l_nsco)):\n",
    "    if(not l_nsco[i][0].isalpha()):\n",
    "        l_nsco[i]=l_nsco[i][1:]\n",
    "    if(not l_nsco[i][-1].isalpha()):\n",
    "        l_nsco[i]=l_nsco[i][:-1]\n",
    "\n",
    "for i in range(len(l_usco)):\n",
    "    if(not l_usco[i][0].isalpha()):\n",
    "        l_usco[i]=l_usco[i][1:]\n",
    "    if(not l_usco[i][-1].isalpha()):\n",
    "        l_usco[i]=l_usco[i][:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_neg=list(set(l_neg))\n",
    "l_unc=list(set(l_unc))\n",
    "l_nsco=list(set(l_nsco))\n",
    "l_usco=list(set(l_usco))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BIO_tagger(text, results):\n",
    "    l_text=word_tokenize(text)\n",
    "    pos=[0,]\n",
    "    l_neg=[]\n",
    "    l_unc=[]\n",
    "    l_starts=[x[\"value\"][\"start\"] for x in results]\n",
    "    l_ends=[x[\"value\"][\"end\"] for x in results]\n",
    "    l_labels=[x[\"value\"][\"labels\"][0] for x in results]\n",
    "    for i in range(len(l_text)-1):\n",
    "        pos.append(pos[i]+len(l_text[i]))\n",
    "        if(text[pos[-2]+len(text[pos[-2]])]==\" \"):\n",
    "            pos[-1]+=1    \n",
    "    for i in range(len(text)):\n",
    "        if(i in pos):\n",
    "            if(i in l_starts):\n",
    "                index=l_starts.index(i)\n",
    "                label=l_labels[index]\n",
    "                if(label==\"NEG\"):\n",
    "                    l_neg.append(\"B\")\n",
    "                    l_unc.append(\"O\")\n",
    "                elif(label==\"NSCO\"):\n",
    "                    l_neg.append(\"I\")\n",
    "                    l_unc.append(\"O\")\n",
    "                elif(label==\"UNC\"):\n",
    "                    l_neg.append(\"O\")\n",
    "                    l_unc.append(\"B\")\n",
    "                elif(label==\"USCO\"):\n",
    "                    l_neg.append(\"O\")\n",
    "                    l_unc.append(\"I\")\n",
    "            else:\n",
    "                x=False\n",
    "                for j in range(len(l_starts)):\n",
    "                    if(l_starts[j]<i and i<l_ends[j]):\n",
    "                        label=l_labels[j]\n",
    "                        if(label==\"NEG\"):\n",
    "                            l_neg.append(\"B\")\n",
    "                            l_unc.append(\"O\")\n",
    "                        elif(label==\"NSCO\"):\n",
    "                            l_neg.append(\"I\")\n",
    "                            l_unc.append(\"O\")\n",
    "                        elif(label==\"UNC\"):\n",
    "                            l_neg.append(\"O\")\n",
    "                            l_unc.append(\"B\")\n",
    "                        elif(label==\"USCO\"):\n",
    "                            l_neg.append(\"O\")\n",
    "                            l_unc.append(\"I\")\n",
    "                        x=True\n",
    "                        break\n",
    "                if not x:\n",
    "                    l_neg.append(\"O\")\n",
    "                    l_unc.append(\"O\")\n",
    "    return [(word, tag) for word, tag in zip(l_text, l_neg)], [(word, tag) for word, tag in zip(l_text, l_unc)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def POS(sentence, word):\n",
    "    \"\"\"\n",
    "    Function that returns the POS tag of a word in a given sentence in Spanish or Catalan.\n",
    "\n",
    "    Args:\n",
    "        sentence (str): The input sentence.\n",
    "        word (str): The word for which the POS tag is needed.\n",
    "\n",
    "    Returns:\n",
    "    str: The POS tag of the word\n",
    "    \"\"\"\n",
    "    doc = None\n",
    "    language = detect(sentence)\n",
    "\n",
    "    if language == \"ca\":\n",
    "        doc = nlp_ca(sentence)\n",
    "    \n",
    "    else:\n",
    "        doc = nlp_es(sentence)\n",
    "\n",
    "    for token in doc:\n",
    "        if token.text.lower() == word.lower():\n",
    "            return token.pos_\n",
    "\n",
    "def init_cap(word):\n",
    "    \"\"\"\n",
    "    Function to check if a word starts with an uppercase letter.\n",
    "\n",
    "    Args:\n",
    "    - word (str): The word to be checked.\n",
    "\n",
    "    Returns:\n",
    "    - bool: True if the word starts with an uppercase letter, False otherwise.\n",
    "    \"\"\"\n",
    "    if word[0].capitalize() == word[0]:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "    \n",
    "def alphanum(word):\n",
    "    \"\"\"\n",
    "    Function to check if a word consists of alphanumeric characters.\n",
    "    \n",
    "    Args:\n",
    "    - word (str): The word to be checked.\n",
    "    \n",
    "    Returns:\n",
    "    - bool: True if the word consists of alphanumeric characters, False otherwise.\n",
    "    \"\"\"\n",
    "    return word.isalnum()\n",
    "\n",
    "def has_num(word):\n",
    "    \"\"\"\n",
    "    Function to check if a word contains a number.\n",
    "    \n",
    "    Args:\n",
    "    - word (str): The word to be checked.\n",
    "    \n",
    "    Returns:\n",
    "    - bool: True if the word contains a number, False otherwise.\n",
    "    \"\"\"\n",
    "    return any(char.isdigit() for char in word)\n",
    "\n",
    "def has_cap(word):\n",
    "    \"\"\"\n",
    "    Function to check if a word contains a capitalized letter.\n",
    "    \n",
    "    Args:\n",
    "    - word (str): The word to be checked.\n",
    "    \n",
    "    Returns:\n",
    "    - bool: True if the word contains a capitalized letter, False otherwise.\n",
    "    \"\"\"\n",
    "    return any(char.isupper() for char in word)\n",
    "\n",
    "def has_dash(word):\n",
    "    \"\"\"\n",
    "    Function to check if a word contains a dash (-).\n",
    "    \n",
    "    Args:\n",
    "    - word (str): The word to be checked.\n",
    "    \n",
    "    Returns:\n",
    "    - bool: True if the word contains a dash, False otherwise.\n",
    "    \"\"\"\n",
    "    return '-' in word\n",
    "\n",
    "def has_us(word):\n",
    "    \"\"\"\n",
    "    Function to check if a word contains an underscore (_).\n",
    "    \n",
    "    Args:\n",
    "    - word (str): The word to be checked.\n",
    "    \n",
    "    Returns:\n",
    "    - bool: True if the word contains an underscore, False otherwise.\n",
    "    \"\"\"\n",
    "    return '_' in word\n",
    "\n",
    "def punctuation(word):\n",
    "    \"\"\"\n",
    "    Function to check if a word contains punctuation.\n",
    "    \n",
    "    Args:\n",
    "    - word (str): The word to be checked.\n",
    "    \n",
    "    Returns:\n",
    "    - bool: True if the word contains punctuation, False otherwise.\n",
    "    \"\"\"\n",
    "    return any(char in string.punctuation for char in word)\n",
    "\n",
    "def suf_n(word, l_sufix):\n",
    "    for i in range(-6,0):\n",
    "        for s in l_sufix:\n",
    "            if(len(s)==(-i) and len(word)>=(-i) and s in word[i:]):\n",
    "                return s\n",
    "    return \"\"\n",
    "\n",
    "def pre_n(word, l_prefix):\n",
    "    for i in range(4,-1,-1):\n",
    "        for p in l_prefix:\n",
    "            if(len(p)==(i+1) and len(word)>=i+1 and p in word[0:i+1]):\n",
    "                return p\n",
    "    return \"\"\n",
    "\n",
    "def bigram_before(sentence, word, max_words=6):\n",
    "    \"\"\"\n",
    "    Function that returns the bigram of up to 6 words before the observed word in a given sentence.\n",
    "\n",
    "    Args:\n",
    "        sentence (str): The input sentence.\n",
    "        word (str): The word up to which the bigrams are needed.\n",
    "        max_words (int): The maximum number of words before the observed word to consider. Default is 6.\n",
    "\n",
    "    Returns:\n",
    "        list: All bigrams before the observed word.\n",
    "    \"\"\"\n",
    "    \n",
    "    doc = None\n",
    "    language = detect(sentence)\n",
    "    if language == \"ca\":\n",
    "        doc = nlp_ca(sentence)\n",
    "    else:\n",
    "        doc = nlp_es(sentence)\n",
    "\n",
    "    bigrams = []\n",
    "    for token in doc:\n",
    "        if token.text.lower() == word.lower():\n",
    "            index = token.i\n",
    "            for i in range(max(0, index - max_words), index):  #Select the maxium len possible.     \n",
    "                if i < index - 1:  \n",
    "                    bigram = [doc[i].text, doc[i + 1].text] \n",
    "                    bigrams.append(bigram) \n",
    "            break\n",
    "\n",
    "    return bigrams\n",
    "\n",
    "def bigram_after(sentence, word):\n",
    "    \"\"\"\n",
    "    Function that returns the POS tag of the word after the observed word in a given sentence.\n",
    "\n",
    "    Args:\n",
    "        sentence (str): The input sentence.\n",
    "        word (str): The word for which the POS tag of the word after it is needed.\n",
    "\n",
    "    Returns:\n",
    "        str: The POS tag of the word after the observed word.\n",
    "    \"\"\"\n",
    "    doc = None\n",
    "    language = detect(sentence)\n",
    "    if language == \"ca\":\n",
    "        doc = nlp_ca(sentence)\n",
    "    else:\n",
    "        doc = nlp_es(sentence)\n",
    "\n",
    "    if str(doc[-1])==\".\": \n",
    "        doc = doc[:-1]\n",
    "\n",
    "    for token in doc:\n",
    "        if token.text.lower() == word.lower():\n",
    "            index = token.i\n",
    "            # Check if there is a word after the observed word\n",
    "            if index + 1 < len(doc):\n",
    "                return [doc[index].text, doc[index+1].text]\n",
    "            else:\n",
    "                return []\n",
    "\n",
    "def before_pos(sentence, word, max_words=6):\n",
    "    \"\"\"\n",
    "    Function that returns the POS tags of up to 6 words before the observed word in a given sentence.\n",
    "\n",
    "    Args:\n",
    "        sentence (str): The input sentence.\n",
    "        word (str): The word for which the POS tags of words before it are needed.\n",
    "        max_words (int): The maximum number of words before the observed word to consider. Default is 6.\n",
    "\n",
    "    Returns:\n",
    "        list: The POS tags of words before the observed word.\n",
    "    \"\"\"\n",
    "    doc = None\n",
    "    language = detect(sentence)\n",
    "\n",
    "    if language == \"ca\":\n",
    "        doc = nlp_ca(sentence)\n",
    "    \n",
    "    else:\n",
    "        doc = nlp_es(sentence)\n",
    "\n",
    "    pos_tags = []\n",
    "    for token in doc:\n",
    "        if token.text.lower() == word.lower():\n",
    "            index = token.i\n",
    "            for i in range(max(0, index - max_words), index):  #Select the maxium len possible.\n",
    "                pos_tags.append(doc[i].pos_)\n",
    "            break\n",
    "\n",
    "    return pos_tags\n",
    "\n",
    "def after_pos(sentence, word):\n",
    "    \"\"\"\n",
    "    Function that returns the POS tag of the word after the observed word in a given sentence.\n",
    "\n",
    "    Args:\n",
    "        sentence (str): The input sentence.\n",
    "        word (str): The word for which the POS tag of the word after it is needed.\n",
    "\n",
    "    Returns:\n",
    "        str: The POS tag of the word after the observed word.\n",
    "    \"\"\"\n",
    "    doc = None\n",
    "    language = detect(sentence)\n",
    "    if language == \"es\":\n",
    "        doc = nlp_es(sentence)\n",
    "    elif language == \"ca\":\n",
    "        doc = nlp_ca(sentence)\n",
    "\n",
    "    for token in doc:\n",
    "        if token.text.lower() == word.lower():\n",
    "            index = token.i\n",
    "            # Check if there is a word after the observed word\n",
    "            if index + 1 < len(doc):\n",
    "                return doc[index + 1].pos_\n",
    "            else:\n",
    "                return []\n",
    "\n",
    "def check_special_words(text, target):\n",
    "    \"\"\"\n",
    "    Verify if a word exist in the special word vocabulary or sequence.\n",
    "\n",
    "    Args:\n",
    "        text (str): text to verify.\n",
    "\n",
    "    Returns:\n",
    "        list: boolean list.\n",
    "    \"\"\"\n",
    "\n",
    "    # Define el conjunto de palabras especiales y secuencias especiales\n",
    "    special_words = {\n",
    "        \"nada\", \"ni\", \"nunca\", \"ningun\", \"ninguno\", \"ninguna\", \"alguna\", \"apenas\",\n",
    "    }\n",
    "    special_sequences = {\n",
    "        \"para nada\", \"ni siquiera\"\n",
    "    }\n",
    " \n",
    "    words = word_tokenize(text)\n",
    "\n",
    "    for i in range(len(words)):\n",
    "        if words[i] == target:\n",
    "            target_idx = i \n",
    "\n",
    "    result = []\n",
    "\n",
    "    next = True\n",
    "    for i in range(len(words)):\n",
    "        word = words[i]\n",
    "        #print(\" \".join(words[i:i+2]),i,len(words))\n",
    "        if next == False:\n",
    "            next= True\n",
    "            pass\n",
    "\n",
    "        elif i < len(words) - 1 and \" \".join(words[i:i+2]) in special_sequences:\n",
    "            #print(\"special sequence:\",len(words) - 1 and \" \".join(words[i:i+2]))\n",
    "            result.append(True)\n",
    "            result.append(True)\n",
    "            next = False\n",
    "            # Skip one word \n",
    "            # i += 1\n",
    "        \n",
    "        elif word in special_words:\n",
    "            result.append(True)\n",
    "        \n",
    "        else:\n",
    "            result.append(False)\n",
    "\n",
    "    res_index = None  \n",
    "    for i in range(len(result)):\n",
    "        if words[i] == target and target_idx == i:\n",
    "            res_index = i\n",
    "            break\n",
    "    return result[res_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractFeatures(text, results, l_sufix, l_prefix):\n",
    "    features={}\n",
    "    l_neg_tagged, l_unc_tagged=BIO_tagger(text,results)\n",
    "    features[\"text_tagged_neg\"]=l_neg_tagged\n",
    "    features[\"text_tagged_unc\"]=l_unc_tagged\n",
    "    features[\"words\"]=[]\n",
    "    for sentence in sent_tokenize(text):\n",
    "        if any(char.isalpha() for char in sentence):\n",
    "            for word in word_tokenize(sentence):\n",
    "                dict_word={}\n",
    "                dict_word[\"POS\"]=POS(sentence, word)\n",
    "                dict_word[\"init_cap\"]=init_cap(word)\n",
    "                dict_word[\"alphanum\"]=alphanum(word)\n",
    "                dict_word[\"has_num\"]=has_num(word)\n",
    "                dict_word[\"has_cap\"]=has_cap(word)\n",
    "                dict_word[\"has_dash\"]=has_dash(word)\n",
    "                dict_word[\"has_us\"]=has_us(word)\n",
    "                dict_word[\"punctuation\"]=punctuation(word)\n",
    "                dict_word[\"suf_n\"]=suf_n(word, l_sufix)\n",
    "                dict_word[\"pre_n\"]=pre_n(word, l_prefix)\n",
    "                dict_word[\"bigram_before\"]=bigram_before(sentence, word)\n",
    "                dict_word[\"bigram_after\"]=bigram_after(sentence, word)\n",
    "                dict_word[\"before_pos\"]=before_pos(sentence, word)\n",
    "                dict_word[\"check_special_words\"]=check_special_words(sentence, word)\n",
    "                features[\"words\"].append({word:dict_word})\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_prefix=[\"a\",\"an\", \"anti\", \"contra\", \"des\", \"dis\", \"de\", \"ex\", \"extra\", \"in\", \"im\", \"i\", \"ante\", \"circum\", \n",
    "            \"circun\", \"entre\", \"inter\", \"infra\", \"pos\", \"post\", \"sub\", \"so\", \"super\", \"sobre\", \"trans\", \"tras\", \n",
    "            \"ultra\", \"vice\", \"ante\", \"pre\", \"archi\", \"extra\", \"hiper\", \"re\", \"super\", \"ultra\", \"bi\", \"bis\", \"mini\", \"mono\", \"mon\", \n",
    "            \"multi\", \"pluri\", \"semi\", \"uni\", \"a\", \"ab\", \"amfi\", \"ana\", \"ante\", \"apo\", \"avant\", \"arxi\", \"ben\", \"auto\", \"bes\",\n",
    "            \"bi\", \"circum\", \"co\", \"cat\", \"cata\", \"contra\", \"de\", \"des\", \"di\", \"dia\", \"en\",\n",
    "            \"endo\", \"eso\", \"ex\", \"exo\", \"extra\", \"eu\", \"hemi\", \"hiper\", \"hipo\", \"fora\", \"in\",\n",
    "            \"infra\", \"inter\", \"intro\", \"macro\", \"mal\", \"mega\", \"meta\", \"micro\", \"menys\", \"mono\", \"multi\",\n",
    "            \"neo\", \"no\", \"pan\", \"par\", \"para\", \"pen\", \"per\", \"peri\", \"pluri\", \"poli\", \"post\", \"pre\" ,\"pro\",\n",
    "            \"prop\", \"proto\", \"pseudo\", \"quasi\", \"re\", \"retro\", \"semi\", \"sin\", \"sota\", \"sub\", \"super\", \n",
    "            \"supra\", \"sus\", \"trans\", \"ultra\", \"vice\"]\n",
    "\n",
    "l_sufix=[\"ón\", \"ote\", \"ota\", \"azo\", \"aza\", \"ato\", \"ata\", \"aca\", \"aco\", \"udo\", \n",
    "         \"uda\", \"arrón\", \"ar\", \"ear\", \"ificar\", \"izar\", \"ecer\", \"ancia\", \"encia\", \n",
    "         \"anza\", \"ción\", \"sión\", \"ismo\", \"dad\", \"tad\", \"ada\", \"ería\", \"aje\", \"ez\", \n",
    "         \"mento\", \"miento\", \"dura\", \"oso\", \"osa\", \"ble\", \"able\", \"ible\", \"enco\", \"enca\", \n",
    "         \"ante\", \"iente\", \"ente\", \"ivo\", \"iva\", \"ano\", \"ana\", \"ado\", \"ada\", \"ido\", \"ida\", \"il\", \n",
    "         \"esco\", \"esca\", \"iento\", \"ienta\", \"oide\", \"izo\", \"ento\", \"mente\", \"aca\", \"ada\", \"al\", \"all\", \"alla\", \"am\", \"ar\", \"ari\", \"at\", \"ea\", \"atge\", \"eda\", \"et\",\n",
    "         \"er\", \"era\", \"à\", \"ana\", \"ista\", \"aire\", \"ia\", \"eria\", \"isme\", \"esa\",\n",
    "         \"eria\", \"or\", \"òria\", \"um\", \"uria\", \"im\", \"ícia\", \"ària\", \"ista\",\n",
    "         \"alla\", \"ció\", \"ment\", \"menta\", \"nça\", \"atge\", \"ó\", \"dor\", \"dora\", \"all\",\n",
    "         \"et\", \"il\", \"ístic\", \"ut\", \"uda\", \"enc\", \"enca\", \"ís\", \"issa\", \"ós\", \"osa\",\n",
    "         \"aire\", \"ble\", \"ívol\", \"ívola\", \"dís\", \"egar\", \"ejar\", \"ificar\", \"itar\", \"itzar\", \"ment\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n"
     ]
    }
   ],
   "source": [
    "test_set=[]\n",
    "i=0\n",
    "for dic in l_test_pro:\n",
    "    test_set.append(extractFeatures(dic[\"text\"], dic[\"results\"], l_sufix, l_prefix))\n",
    "    i+=1\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"full_test_set_CRF.json\",\"w\") as f:\n",
    "  json.dump(test_set,f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NaturalLanguageProcessing",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
