{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f=open(\"test.json\")\n",
    "test_set=json.load(f)\n",
    "\n",
    "f=open(\"training.json\")\n",
    "training_set=json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f=open(\"full_test_set_CRF.json\")\n",
    "test_set_features=json.load(f)\n",
    "\n",
    "f=open(\"full_training_set_CRF.json\")\n",
    "training_set_features=json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_training=[]\n",
    "for i in range(len(training_set)):\n",
    "    l_training.append({\"text\":training_set[i][\"data\"][\"text\"], \"results\":training_set[i][\"predictions\"][0][\"result\"]})\n",
    "\n",
    "l_test=[]\n",
    "for i in range(len(test_set)):\n",
    "    l_test.append({\"text\":test_set[i][\"data\"][\"text\"], \"results\":test_set[i][\"predictions\"][0][\"result\"]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eliminate_patient_information(string):\n",
    "    index = string.find(\"informe\")\n",
    "    if index != -1:\n",
    "        return string[index:], index\n",
    "    else:\n",
    "        return string, 0\n",
    "\n",
    "def eliminate_final_information(string):\n",
    "    index = string.find(\"destinacio a l'alta\")\n",
    "    if index != -1:\n",
    "        return string[:index]\n",
    "    else:\n",
    "        return string\n",
    "\n",
    "def nhc_lopd(string):\n",
    "    nhc_indices = [i for i in range(len(string)) if string.startswith(\"nhc\", i)]\n",
    "    lopd_indices = [i+len(\"lopd\") for i in range(len(string)) if string.startswith(\"lopd\", i)]\n",
    "\n",
    "    results = []\n",
    "    for nhc_index, lopd_index in zip(nhc_indices, lopd_indices):\n",
    "        if nhc_index < lopd_index:\n",
    "            length = lopd_index - nhc_index \n",
    "            results.append((nhc_index, length))\n",
    "            break  # break inner loop, move to next nhc_index\n",
    "\n",
    "    return results\n",
    "\n",
    "def correct_results(results, lenght, nhclopd):\n",
    "    for i in range(len(results)):\n",
    "        results[i][\"value\"][\"start\"]-=lenght\n",
    "        results[i][\"value\"][\"end\"]-=lenght\n",
    "        for j in nhclopd:\n",
    "            if(j[0]<results[i][\"value\"][\"start\"]):\n",
    "                results[i][\"value\"][\"start\"]-=j[1]\n",
    "                results[i][\"value\"][\"end\"]-=j[1]\n",
    "    return results\n",
    "\n",
    "def preprocessing(text_dict):\n",
    "    text=text_dict[\"text\"]\n",
    "    results=text_dict[\"results\"]\n",
    "    text, lenght=eliminate_patient_information(text)\n",
    "    text=eliminate_final_information(text)\n",
    "    nhclopd=nhc_lopd(text)\n",
    "    for i in (nhclopd):\n",
    "        index1=i[0]\n",
    "        index2=i[0]+i[1]\n",
    "        text=text[0:index1]+text[index2:]\n",
    "    results=correct_results(results, lenght, nhclopd)\n",
    "    text_dict[\"text\"]=text\n",
    "    text_dict[\"results\"]=results\n",
    "    return text_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_training_pro=[]\n",
    "for i in range(len(l_training)):\n",
    "    l_training_pro.append(preprocessing(l_training[i]))\n",
    "\n",
    "l_test_pro=[]\n",
    "for i in range(len(l_test)):\n",
    "    l_test_pro.append(preprocessing(l_test[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tagger(text, results):\n",
    "    l_text=word_tokenize(text)\n",
    "    for i in range(len(l_text)):\n",
    "        if(\"``\" in l_text[i] or \"''\" in l_text[i]):\n",
    "            l_text[i]=\"\\\"\"\n",
    "    pos=[0,]\n",
    "    tags=[]\n",
    "    l_starts=[x[\"value\"][\"start\"] for x in results]\n",
    "    l_ends=[x[\"value\"][\"end\"] for x in results]\n",
    "    l_labels=[x[\"value\"][\"labels\"][0] for x in results]\n",
    "    for i in range(1,len(l_text)):\n",
    "        j=pos[-1]+len(l_text[i-1])\n",
    "        if(l_text[i]!=\"\\\"\"):\n",
    "            while not text.startswith(l_text[i],j):\n",
    "                j+=1\n",
    "        pos.append(j)\n",
    "    for i in range(len(text)):\n",
    "        if(i in pos):\n",
    "            if(i in l_starts):\n",
    "                index=l_starts.index(i)\n",
    "                label=l_labels[index]\n",
    "                tags.append(label)\n",
    "            else:\n",
    "                x=False\n",
    "                for j in range(len(l_starts)):\n",
    "                    if(l_starts[j]<i and i<l_ends[j]):\n",
    "                        label=l_labels[j]\n",
    "                        tags.append(label)\n",
    "                        x=True\n",
    "                        break\n",
    "                if not x:\n",
    "                    tags.append(\"O\")\n",
    "    return [(word, tag) for word, tag in zip(l_text, tags)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dic in test_set_features:\n",
    "    dic[\"text_tagged\"]=tagger(dic[\"text\"], dic[\"results\"])\n",
    "\n",
    "for dic in training_set_features:\n",
    "    dic[\"text_tagged\"]=tagger(dic[\"text\"], dic[\"results\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dic1, dic2 in zip(l_test_pro, test_set_features):\n",
    "    dic1[\"words\"]=dic2[\"words\"]\n",
    "\n",
    "for dic1, dic2 in zip(l_training_pro, training_set_features):\n",
    "    dic1[\"words\"]=dic2[\"words\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"full_test_set_CRF_tagged.json\",\"w\") as f:\n",
    "  json.dump(l_test_pro,f)\n",
    "\n",
    "with open(\"full_training_set_CRF_tagged.json\",\"w\") as f:\n",
    "  json.dump(l_training_pro,f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
