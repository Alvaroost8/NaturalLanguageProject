{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Character Embedding Representation Notebook\n",
    "\n",
    "This notebook focuses on creating vector representations of words using a pre-trained FastText model, specifically the 'fasttext-sbwc.3.6.e20.vec' model. The primary objective is to convert words from training texts into their corresponding vector forms, which have a fixed length of 300 dimensions. This representation is crucial for the task described in the paper \"Extending a Deep Learning Approach for Negation Cues Detection in Spanish.\"\n",
    "\n",
    "## Overview\n",
    "The notebook includes the following key steps:\n",
    "1. **Loading Data**: Import training and test datasets from JSON files (`full_training_set_CRF_tagged.json` and `full_test_set_CRF_tagged.json`).\n",
    "2. **Loading Pre-trained Model**: Load the pre-trained FastText word vectors (`fasttext-sbwc.3.6.e20.vec`).\n",
    "3. **Word Vector Conversion**: Define functions to convert words into their corresponding vector representations using the pre-trained model.\n",
    "\n",
    "## Files and Resources\n",
    "- **Pre-trained FastText Model**: `fasttext-sbwc.3.6.e20.vec`, downloaded from [FastText](https://fasttext.cc/).\n",
    "- **Training and Test Data**: `full_training_set_CRF_tagged.json` and `full_test_set_CRF_tagged.json`, containing the data for training and evaluation.\n",
    "\n",
    "## Goal\n",
    "The goal is to leverage pre-trained embeddings to represent words in a high-dimensional space, facilitating the analysis and processing of textual data for tasks such as negation cues detection in the Spanish language.\n",
    "\n",
    "By the end of this notebook, you will have a set of word vectors ready for use in further natural language processing tasks, particularly those involving character-level analysis as mentioned in the referenced paper.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "from nltk import sent_tokenize\n",
    "from nltk import word_tokenize\n",
    "\n",
    "f1 = open(\"full_training_set_CRF_tagged.json\")\n",
    "full_training_set_CRF = json.load(f1)\n",
    "\n",
    "f1 = open(\"full_test_set_CRF_tagged.json\")\n",
    "full_test_set_CRF = json.load(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "wordvectors_file_vec = 'fasttext-sbwc.3.6.e20.vec'\n",
    "cantidad = 100000\n",
    "wordfast_model = KeyedVectors.load_word2vec_format(wordvectors_file_vec, limit=cantidad)\n",
    "\n",
    "def get_word_vector(word):\n",
    "    \"\"\"\n",
    "    Esta función toma una palabra como entrada y devuelve su vector correspondiente\n",
    "    utilizando el modelo de FastText previamente cargado.\n",
    "    \n",
    "    :param word: Palabra para la que se desea obtener el vector.\n",
    "    :return: Vector de la palabra si está en el vocabulario del modelo, de lo contrario None.\n",
    "    \"\"\"\n",
    "    if word in wordfast_model:\n",
    "        return wordfast_model[word]\n",
    "    else:\n",
    "        print(f\"La palabra '{word}' no está en el vocabulario.\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_extraction_wordfast(document,n):\n",
    "    \"Receive a document and the number of the document. Then it returns the labels divided in sentences\"    \n",
    "    text = document['text']\n",
    "    tagged_sentences = []\n",
    "    tag_index = 0\n",
    "    \n",
    "    for sentence in sent_tokenize(text):\n",
    "        if(any(char.isalpha() for char in sentence)):\n",
    "            l = []\n",
    "            for word in word_tokenize(sentence):\n",
    "                 l.append(get_word_vector(word))\n",
    "                 tag_index += 1\n",
    "            tagged_sentences.append(l)\n",
    "\n",
    "    return tagged_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectors_fast = []\n",
    "for i in range(len(full_training_set_CRF)):\n",
    "    word_vectors_fast += word_extraction_wordfast(full_training_set_CRF[i],i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('w_fast_vectors', 'w', encoding='utf-8') as f:\n",
    "    json.dump(word_vectors_fast, f, ensure_ascii=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NaturalLanguageProcessing",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
