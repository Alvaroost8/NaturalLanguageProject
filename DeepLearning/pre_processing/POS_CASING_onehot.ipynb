{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# POS and Casing One-Hot Encoding Notebook\n",
    "\n",
    "This notebook focuses on transforming Part-of-Speech (POS) tags and word casing information into one-hot encoded vectors. The primary objective is to convert POS tags into 17-length one-hot encodings and word casing into 8-length one-hot encodings. This transformation is essential for the deep-learning model.\n",
    "## Overview\n",
    "The notebook includes the following key steps:\n",
    "1. **Loading Data**: Import training data containing POS tags and casing information.\n",
    "2. **One-Hot Encoding Transformation**: Convert POS tags into 17-length one-hot encoded vectors and casing information into 8-length one-hot encoded vectors.\n",
    "3. **Handling Sentence Separation**: Ensure that the transformed data maintains the structure of individual sentences, addressing the issue where words were initially concatenated together without sentence separation.\n",
    "\n",
    "## Problem Addressed\n",
    "While implementing the one-hot encoding transformations, an issue was encountered where all words were concatenated into a single sequence without maintaining sentence boundaries. To resolve this, additional steps were taken to separate and correctly structure the transformed data by sentences.\n",
    "\n",
    "## Goal\n",
    "The main goal of this notebook is to prepare POS and casing information in a format that can be readily used for training deep learning models. By the end of this notebook, you will have lists where POS tags and casing information are represented as one-hot encoded vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_json(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    return data\n",
    "\n",
    "training = load_json(\"full_training_set_CRF_tagged.json\")\n",
    "test = load_json(\"full_test_set_CRF_tagged.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_tags = set()\n",
    "for i in range(len(training)):\n",
    "    training[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "# Crear un set para almacenar las etiquetas POS\n",
    "pos_tags = set()\n",
    "\n",
    "# Iterar sobre cada palabra y su etiqueta POS\n",
    "for i in range(len(training)):\n",
    "    for word_info in training[i]['words']:\n",
    "        for word, attributes in word_info.items():\n",
    "            pos_tag = attributes.get('POS')\n",
    "            if pos_tag is not None:  # Verificar que la etiqueta POS no sea None\n",
    "                pos_tags.add(pos_tag)\n",
    "\n",
    "# Iterar sobre cada palabra y su etiqueta POS\n",
    "for i in range(len(test)):\n",
    "    for word_info in test[i]['words']:\n",
    "        for word, attributes in word_info.items():\n",
    "            pos_tag = attributes.get('POS')\n",
    "            if pos_tag is not None:  # Verificar que la etiqueta POS no sea None\n",
    "                pos_tags.add(pos_tag)\n",
    "\n",
    "# Convertir el set a una lista\n",
    "pos_tags = list(pos_tags)\n",
    "\n",
    "# Crear un diccionario para mapear cada etiqueta POS a un índice\n",
    "pos_to_index = {pos: idx for idx, pos in enumerate(pos_tags)}\n",
    "\n",
    "# Función para convertir etiquetas POS a one-hot\n",
    "def pos_to_one_hot(pos, pos_to_index):\n",
    "    one_hot = [0] * len(pos_to_index)\n",
    "    if pos in pos_to_index:\n",
    "        one_hot[pos_to_index[pos]] = 1\n",
    "    return one_hot\n",
    "\n",
    "# Convertir todas las etiquetas POS en el conjunto de entrenamiento a one-hot\n",
    "one_hot_encoded_pos_train = []\n",
    "\n",
    "for i in range(len(training)):\n",
    "    for word_info in training[i]['words']:\n",
    "        for word, attributes in word_info.items():\n",
    "            pos_tag = attributes.get('POS')\n",
    "            if pos_tag is not None:\n",
    "                one_hot_vector = pos_to_one_hot(pos_tag, pos_to_index)\n",
    "                one_hot_encoded_pos_train.append(one_hot_vector)\n",
    "            else:\n",
    "                one_hot_encoded_pos_train.append([0] * len(pos_to_index))\n",
    "\n",
    "# Verificar los vectores one-hot\n",
    "print(one_hot_encoded_pos_train[0])\n",
    "\n",
    "\n",
    "# Convertir todas las etiquetas POS en el conjunto de entrenamiento a one-hot\n",
    "one_hot_encoded_pos_test = []\n",
    "\n",
    "for i in range(len(test)):\n",
    "    for word_info in test[i]['words']:\n",
    "        for word, attributes in word_info.items():\n",
    "            pos_tag = attributes.get('POS')\n",
    "            if pos_tag is not None:\n",
    "                one_hot_vector = pos_to_one_hot(pos_tag, pos_to_index)\n",
    "                one_hot_encoded_pos_test.append(one_hot_vector)\n",
    "            else:\n",
    "                one_hot_encoded_pos_test.append([0]*len(pos_to_index))\n",
    "                \n",
    "# Verificar los vectores one-hot\n",
    "print(one_hot_encoded_pos_test[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import sent_tokenize\n",
    "from nltk import word_tokenize\n",
    "\n",
    "def word_extraction(document,n):\n",
    "    \"Receive a document and the number of the document. Then it returns the labels divided in sentences\"    \n",
    "    text = document['text']\n",
    "    tagged_sentences = []\n",
    "    tag_index = 0\n",
    "    \n",
    "    for sentence in sent_tokenize(text):\n",
    "        if(any(char.isalpha() for char in sentence)):\n",
    "            l = []\n",
    "            for word in word_tokenize(sentence):\n",
    "                 l.append(word)\n",
    "                 tag_index += 1\n",
    "            tagged_sentences.append(l)\n",
    "\n",
    "    return tagged_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_words_train = []\n",
    "for i in range(len(training)):\n",
    "    l_words_train += word_extraction(training[i],i)\n",
    "\n",
    "l_words_test = []\n",
    "for i in range(len(test)):\n",
    "    l_words_test += word_extraction(test[i],i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['informe', \"d'alta\", \"d'hospitalitzacio\", 'motiu', \"d'ingres\", 'paciente', 'que', 'ingresa', 'de', 'forma', 'programada', 'para', 'realizacion', 'de', 'uretrotomia', 'interna', '.']\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "205114\n",
      "205114\n"
     ]
    }
   ],
   "source": [
    "c= 0\n",
    "print(len(one_hot_encoded_pos_train))\n",
    "for sentence in l_words_train:\n",
    "    for word in sentence:\n",
    "        c+=1\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56\n",
      "56\n",
      "8\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "pos_train_separated = []\n",
    "index = 0\n",
    "\n",
    "for sentence in l_words_train:\n",
    "    sentence_length = len(sentence)\n",
    "    pos_train_separated.append(one_hot_encoded_pos_train[index:index+sentence_length])\n",
    "    index += sentence_length\n",
    "\n",
    "print(len(pos_train_separated[44]))\n",
    "print(len(l_words_train[44]))\n",
    "\n",
    "\n",
    "pos_test_separated = []\n",
    "index = 0\n",
    "\n",
    "for sentence in l_words_test:\n",
    "    sentence_length = len(sentence)\n",
    "    pos_test_separated.append(one_hot_encoded_pos_train[index:index+sentence_length])\n",
    "    index += sentence_length\n",
    "\n",
    "print(len(pos_test_separated[44]))\n",
    "print(len(l_words_test[44]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3211\n",
      "11782\n",
      "3211\n",
      "11782\n"
     ]
    }
   ],
   "source": [
    "print(len(pos_test_separated))\n",
    "print(len(pos_train_separated))\n",
    "\n",
    "print(len(l_words_test))\n",
    "print(len(l_words_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"def save_json(data, file_path):\\n    with open(file_path, 'w') as f:\\n        json.dump(data, f)\\n\\nsave_json(pos_train_separated,'train_pos_onehot.json')\\nsave_json(pos_test_separated,'test_pos_onehot.json')\\n\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def save_json(data, file_path):\n",
    "    with open(file_path, 'w') as f:\n",
    "        json.dump(data, f)\n",
    "\n",
    "save_json(pos_train_separated,'train_pos_onehot.json')\n",
    "save_json(pos_test_separated,'test_pos_onehot.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_to_res(vector,attribute):\n",
    "    if attribute == True:\n",
    "        vector.append(1)\n",
    "    else:\n",
    "        vector.append(0)\n",
    "\n",
    "    return vector\n",
    "\n",
    "# Convertir todas las etiquetas POS en el conjunto de entrenamiento a one-hot\n",
    "one_hot_encoded_casing_train = []\n",
    "\n",
    "for i in range(len(training)):\n",
    "    for word_info in training[i]['words']:\n",
    "        for word, attributes in word_info.items():\n",
    "            vector = []\n",
    "            vector = add_to_res(vector,attributes.get('init_cap'))\n",
    "            vector = add_to_res(vector,attributes.get('alpahnum'))\n",
    "            vector = add_to_res(vector,attributes.get('has_num'))\n",
    "            vector = add_to_res(vector,attributes.get('has_cap'))\n",
    "            vector = add_to_res(vector,attributes.get('has_dash'))\n",
    "            vector = add_to_res(vector,attributes.get('has_us'))\n",
    "            vector = add_to_res(vector,attributes.get('punctuation'))\n",
    "            vector = add_to_res(vector,attributes.get('check_special_words'))\n",
    "            one_hot_encoded_casing_train.append(vector)\n",
    "\n",
    "one_hot_encoded_casing_test = []\n",
    "\n",
    "for i in range(len(test)):\n",
    "    for word_info in test[i]['words']:\n",
    "        for word, attributes in word_info.items():\n",
    "            vector = []\n",
    "            vector = add_to_res(vector,attributes.get('init_cap'))\n",
    "            vector = add_to_res(vector,attributes.get('alpahnum'))\n",
    "            vector = add_to_res(vector,attributes.get('has_num'))\n",
    "            vector = add_to_res(vector,attributes.get('has_cap'))\n",
    "            vector = add_to_res(vector,attributes.get('has_dash'))\n",
    "            vector = add_to_res(vector,attributes.get('has_us'))\n",
    "            vector = add_to_res(vector,attributes.get('punctuation'))\n",
    "            vector = add_to_res(vector,attributes.get('check_special_words'))\n",
    "            one_hot_encoded_casing_test.append(vector)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56\n",
      "56\n",
      "8\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "casing_train_separated = []\n",
    "index = 0\n",
    "\n",
    "for sentence in l_words_train:\n",
    "    sentence_length = len(sentence)\n",
    "    casing_train_separated.append(one_hot_encoded_casing_train[index:index+sentence_length])\n",
    "    index += sentence_length\n",
    "\n",
    "print(len(casing_train_separated[44]))\n",
    "print(len(l_words_train[44]))\n",
    "\n",
    "\n",
    "casing_test_separated = []\n",
    "index = 0\n",
    "\n",
    "for sentence in l_words_test:\n",
    "    sentence_length = len(sentence)\n",
    "    casing_test_separated.append(one_hot_encoded_casing_test[index:index+sentence_length])\n",
    "    index += sentence_length\n",
    "\n",
    "print(len(casing_test_separated[44]))\n",
    "print(len(l_words_test[44]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"def save_json(data, file_path):\\n    with open(file_path, 'w') as f:\\n        json.dump(data, f)\\n\\nsave_json(casing_train_separated,'train_casing_onehot.json')\\nsave_json(casing_test_separated,'test_casing_onehot.json')\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def save_json(data, file_path):\n",
    "    with open(file_path, 'w') as f:\n",
    "        json.dump(data, f)\n",
    "\n",
    "save_json(casing_train_separated,'train_casing_onehot.json')\n",
    "save_json(casing_test_separated,'test_casing_onehot.json')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NaturalLanguageProcessing",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
