{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# List Structure Processing\n",
    "\n",
    "This notebook focuses on processing structured data lists, including word embeddings, casing information, and POS tags, to prepare them for deep learning tasks. The primary objective is to handle ensure the same lenght of all the entries.\n",
    "\n",
    "## Overview\n",
    "1. **Loading Libraries and Data**: Import necessary libraries and multiple datasets, including word embeddings, casing, and POS tags.\n",
    "2. **Data Transformation**: Process the loaded data to handle specific structural requirements, such as one-hot encoding and ensuring consistency across different data elements.\n",
    "\n",
    "## Goal\n",
    "The main goal of this notebook is to organize and transform structured data lists into formats suitable for deep learning. By the end of the notebook, you will have preprocessed data ready for training and testing machine learning models, with correct handling of word embeddings, casing, and POS tag information.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import copy\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "f=open(\"train_word_embeddings_reduced.json\")\n",
    "train_word_embeddings=json.load(f)\n",
    "\n",
    "f=open(\"test_word_embeddings_reduced.json\")\n",
    "test_word_embeddings=json.load(f)\n",
    "\n",
    "f=open(\"train_casing_onehot.json\")\n",
    "train_casing_onehot=json.load(f)\n",
    "\n",
    "f=open(\"test_casing_onehot.json\")\n",
    "test_casing_onehot=json.load(f)\n",
    "\n",
    "f=open(\"train_pos_onehot.json\")\n",
    "train_pos_onehot=json.load(f)\n",
    "\n",
    "f=open(\"test_pos_onehot.json\")\n",
    "test_pos_onehot=json.load(f)\n",
    "\n",
    "f=open(\"full_training_set_CRF_tagged.json\")\n",
    "training_set=json.load(f)\n",
    "\n",
    "f=open(\"full_test_set_CRF_tagged.json\")\n",
    "test_set=json.load(f)\n",
    "\n",
    "f=open(\"y_train.json\")\n",
    "y_train=json.load(f)\n",
    "\n",
    "f=open(\"y_test.json\")\n",
    "y_test=json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_casing_onehot' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m leng\u001b[38;5;241m=\u001b[39m[]\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sentence \u001b[38;5;129;01min\u001b[39;00m train_casing_onehot:\n\u001b[0;32m      4\u001b[0m     leng\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mlen\u001b[39m(sentence))\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sentence \u001b[38;5;129;01min\u001b[39;00m test_casing_onehot:\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_casing_onehot' is not defined"
     ]
    }
   ],
   "source": [
    "leng=[]\n",
    "\n",
    "for sentence in train_casing_onehot:\n",
    "    leng.append(len(sentence))\n",
    "\n",
    "for sentence in test_casing_onehot:\n",
    "    leng.append(len(sentence))\n",
    "\n",
    "print(max(leng))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_struct(l, length, length_vector):\n",
    "    l_copy=copy.deepcopy(l)\n",
    "    for l2 in l_copy:\n",
    "        while len(l2)<length:\n",
    "            l2.append([0.]*length_vector)\n",
    "    return l_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_struct2(l, length):\n",
    "    l_copy=copy.deepcopy(l)\n",
    "    for l2 in l_copy:\n",
    "        while len(l2)<length:\n",
    "            l2.append(\"PAD\")\n",
    "    return l_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_casing_onehot2=list_struct(train_casing_onehot, 377, 8)\n",
    "test_casing_onehot2=list_struct(test_casing_onehot, 377, 8)\n",
    "\n",
    "train_pos_onehot2=list_struct(train_pos_onehot, 377, 17)\n",
    "test_pos_onehot2=list_struct(test_pos_onehot, 377, 17)\n",
    "\n",
    "train_word_embeddings2=list_struct(train_word_embeddings, 377, 100)\n",
    "test_word_embeddings2=list_struct(test_word_embeddings, 377, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train2=list_struct2(y_train, 377)\n",
    "y_test2=list_struct2(y_test, 377)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numerical_tags(dict_index_tag, y):\n",
    "    tags=[]\n",
    "    for sentence in y:\n",
    "        sentence_tags=[]\n",
    "        for tag in sentence:\n",
    "            l=[0.]*6\n",
    "            l[dict_index_tag[tag]]=1.\n",
    "            sentence_tags.append(l)\n",
    "        tags.append(sentence_tags)\n",
    "    return tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_index_tag={\"O\":0, \"NEG\":1, \"NSCO\":2, \"UNC\":3, \"USCO\":4, \"PAD\":5}\n",
    "dict_index_tag_inverted={0:\"O\", 1:\"NEG\", 2:\"NSCO\", 3:\"UNC\", 4:\"USCO\", 5:\"PAD\"}\n",
    "\n",
    "y_train3=numerical_tags(dict_index_tag, y_train2)\n",
    "y_test3=numerical_tags(dict_index_tag, y_test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"train_casing_onehot.json\",\"w\") as f:\n",
    "  json.dump(train_casing_onehot2, f)\n",
    "\n",
    "with open(\"test_casing_onehot.json\",\"w\") as f:\n",
    "  json.dump(test_casing_onehot2, f)\n",
    "\n",
    "with open(\"train_pos_onehot.json\",\"w\") as f:\n",
    "  json.dump(train_pos_onehot2, f)\n",
    "\n",
    "with open(\"test_pos_onehot.json\",\"w\") as f:\n",
    "  json.dump(test_pos_onehot2, f)\n",
    "\n",
    "with open(\"train_word_embeddings_reduced.json\",\"w\") as f:\n",
    "  json.dump(train_word_embeddings2, f)\n",
    "\n",
    "with open(\"test_word_embeddings_reduced.json\",\"w\") as f:\n",
    "  json.dump(test_word_embeddings2, f)\n",
    "\n",
    "with open(\"y_train_numerical.json\",\"w\") as f:\n",
    "  json.dump(y_train3, f)\n",
    "\n",
    "with open(\"y_test_numerical.json\",\"w\") as f:\n",
    "  json.dump(y_test3, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
