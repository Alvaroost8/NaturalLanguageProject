{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embedding with Word2Vec\n",
    "\n",
    "This notebook demonstrates the process of loading pre-trained Word2Vec embeddings and applying them to a training and test dataset. The primary objective is to convert words from training texts into their corresponding vector representations using a pre-trained Word2Vec model.\n",
    "\n",
    "## Overview\n",
    "1. **Loading Libraries and Data**: Import necessary libraries and datasets.\n",
    "2. **Loading Pre-trained Word2Vec Model**: Load the Spanish Billion Words Corpus (SBWC) Word2Vec model.\n",
    "3. **Embedding Transformation**: Transform the words in the training and test datasets into their respective Word2Vec vector representations.\n",
    "\n",
    "## Goal\n",
    "The main goal of this notebook is to prepare word embeddings for use in various natural language processing tasks. By the end of the notebook, you will have word vectors ready for integration into downstream models and analyses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "import gensim\n",
    "import gensim.downloader as api\n",
    "from nltk import sent_tokenize, word_tokenize\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path='http://cs.famaf.unc.edu.ar/~ccardellino/SBWCE/SBW-vectors-300-min5.bin.gz'\n",
    "word2vec_model=gensim.models.KeyedVectors.load_word2vec_format(model_path, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "f=open(\"full_training_set_CRF_tagged.json\")\n",
    "training_set=json.load(f)\n",
    "\n",
    "f=open(\"full_test_set_CRF_tagged.json\")\n",
    "test_set=json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_extraction(document):   \n",
    "    text=document['text']\n",
    "    tagged_sentences=[]\n",
    "    tag_index=0\n",
    "    \n",
    "    for sentence in sent_tokenize(text):\n",
    "        if(any(char.isalpha() for char in sentence)):\n",
    "            l = []\n",
    "            for word in word_tokenize(sentence):\n",
    "                 l.append(word)\n",
    "                 tag_index += 1\n",
    "            tagged_sentences.append(l)\n",
    "\n",
    "    return tagged_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set_words = []\n",
    "for i in range(len(test_set)):\n",
    "    test_set_words += word_extraction(test_set[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3211\n"
     ]
    }
   ],
   "source": [
    "print(len(test_set_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_embeddings=[]\n",
    "for sentence in test_set_words:\n",
    "    l=[]\n",
    "    for word in sentence:\n",
    "        if word in word2vec_model:\n",
    "            l.append(word2vec_model.get_vector(word))\n",
    "        else:\n",
    "            l.append(np.zeros(shape=(300,)))\n",
    "    word_embeddings.append(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sentence in word_embeddings:\n",
    "    for i in range(len(sentence)):\n",
    "        sentence[i]=sentence[i].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"test_word_embeddings.json\",\"w\") as f:\n",
    "  json.dump(word_embeddings, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
