{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimension Reduction Notebook\n",
    "\n",
    "This notebook is designed to reduce the dimensionality of high-dimensional embeddings. High-dimensional embeddings can be computationally heavy and challenging to work with, particularly for visualization and further analysis. By reducing the dimensionality, we aim to retain as much relevant information as possible while simplifying the dataset for more efficient processing.\n",
    "\n",
    "## Parameters\n",
    "- **input_file**: Path to the JSON file containing the original high-dimensional embeddings.\n",
    "- **output_file**: Path to the JSON file where the reduced-dimensionality embeddings will be saved.\n",
    "- **n_components**: Number of dimensions to reduce the data to (default is 100).\n",
    "\n",
    "## Functionality\n",
    "The notebook performs the following steps:\n",
    "1. **Data Loading**: Loads the high-dimensional embeddings from the specified JSON file.\n",
    "2. **Flattening Embeddings**: Converts the nested structure of embeddings into a flat array for processing.\n",
    "3. **Dimensionality Reduction**: Applies Principal Component Analysis (PCA) to reduce the number of dimensions to the specified `n_components`.\n",
    "4. **Unflattening Embeddings**: Converts the reduced-dimensionality embeddings back to their original nested structure.\n",
    "5. **Data Saving**: Saves the reduced-dimensionality embeddings to the specified output JSON file.\n",
    "\n",
    "## Techniques Used\n",
    "- **Principal Component Analysis (PCA)**: A statistical procedure that uses orthogonal transformation to convert a set of observations of possibly correlated variables into a set of values of linearly uncorrelated variables called principal components.\n",
    "\n",
    "## Goal\n",
    "The main goal of this notebook is to reduce the computational load and complexity associated with high-dimensional embeddings, making them more manageable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flattening embeddings...\n",
      "Original shape: (205114, 300)\n",
      "Reducing dimensionality...\n",
      "Reduced shape: (205114, 100)\n",
      "Reconstructing data structure...\n",
      "Saving reduced data...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "import copy\n",
    "\n",
    "def load_json(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    return data\n",
    "\n",
    "def save_json(data, file_path):\n",
    "    with open(file_path, 'w') as f:\n",
    "        json.dump(data, f)\n",
    "\n",
    "def flatten_embeddings(data):\n",
    "    flat_embeddings = []\n",
    "    for sentence in data:\n",
    "        for word in sentence:\n",
    "            flat_embeddings.append(word)\n",
    "    return np.array(flat_embeddings)\n",
    "\n",
    "def unflatten_embeddings(flat_embeddings, original_data):\n",
    "    index = 0\n",
    "    new_data = []\n",
    "    for sentence in original_data:\n",
    "        new_sentence = []\n",
    "        for _ in sentence:\n",
    "            new_sentence.append(flat_embeddings[index].tolist())\n",
    "            index += 1\n",
    "        new_data.append(new_sentence)\n",
    "    return new_data\n",
    "\n",
    "def reduce_dimensionality(data, n_components=100):\n",
    "    pca = PCA(n_components=n_components)\n",
    "    reduced_data = pca.fit_transform(data)\n",
    "    return reduced_data\n",
    "\n",
    "def main(input_file, output_file, n_components=100):\n",
    "    print(\"Loading data...\")\n",
    "    data = load_json(input_file)\n",
    "    print(\"Flattening embeddings...\")\n",
    "    flat_embeddings = flatten_embeddings(data)\n",
    "    print(f\"Original shape: {flat_embeddings.shape}\")\n",
    "\n",
    "    print(\"Reducing dimensionality...\")\n",
    "    reduced_embeddings = reduce_dimensionality(flat_embeddings, n_components)\n",
    "    print(f\"Reduced shape: {reduced_embeddings.shape}\")\n",
    "\n",
    "    print(\"Reconstructing data structure...\")\n",
    "    new_data = unflatten_embeddings(reduced_embeddings, data)\n",
    "\n",
    "    print(\"Saving reduced data...\")\n",
    "    save_json(new_data, output_file)\n",
    "    print(\"Done!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    input_file = 'word_embeddings.json'\n",
    "    output_file = 'word_embeddings_reduced.json'\n",
    "    main(input_file, output_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11782\n",
      "17\n",
      "100\n",
      "11782\n",
      "17\n",
      "300\n"
     ]
    }
   ],
   "source": [
    "data_reduced = load_json('word_embeddings_reduced.json')\n",
    "print(len(data_reduced))\n",
    "print(len(data_reduced[0]))\n",
    "print(len(data_reduced[0][0]))\n",
    "\n",
    "data_reduced = load_json('word_embeddings.json')\n",
    "print(len(data_reduced))\n",
    "print(len(data_reduced[0]))\n",
    "print(len(data_reduced[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import json\n",
    "\n",
    "def load_json(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    return data\n",
    "\n",
    "def save_json(data, file_path):\n",
    "    with open(file_path, 'w') as f:\n",
    "        json.dump(data, f)\n",
    "\n",
    "def list_struct(l, length, length_vector):\n",
    "    l_copy=copy.deepcopy(l)\n",
    "    for l2 in l_copy:\n",
    "        while len(l2)<length:\n",
    "            l2.append([0]*length_vector)\n",
    "    return l_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "fast_vector_train = load_json(\"w_fast_vectors_reduced.json\")\n",
    "fast_vector_test = load_json(\"w_fast_vectors_test_reduced.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17\n"
     ]
    }
   ],
   "source": [
    "print(len(fast_vector_train[0]))\n",
    "fast_vector_train = list_struct(fast_vector_train,377,100)\n",
    "fast_vector_test = list_struct(fast_vector_test,377,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_json(fast_vector_train,'w_fast_vectors_reduced.json')\n",
    "save_json(fast_vector_test,'w_fast_vectors_test_reduced.json')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NaturalLanguageProcessing",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
