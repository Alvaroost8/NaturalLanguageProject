{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Description\n",
    "\n",
    "## Purpose\n",
    "This notebook implements a deep neural network model to identify negation in text. It involves loading preprocessed data, defining a custom dataset, building the neural network model, and training it using PyTorch.\n",
    "\n",
    "## Notebook Content\n",
    "\n",
    "### 1. Dependency Installation\n",
    "Installs and imports necessary libraries for the project, including:\n",
    "- `torch` for building and training the neural network.\n",
    "- `numpy` for numerical operations.\n",
    "- `json` for loading and saving data.\n",
    "\n",
    "### 2. Data Loading\n",
    "Loads preprocessed data from JSON files. The data includes word embeddings, casing information, part-of-speech (POS) tags, and labels.\n",
    "\n",
    "### 3. Dataset Definition\n",
    "Defines a custom dataset class using PyTorch's `Dataset` and `DataLoader` to handle the training and validation data.\n",
    "\n",
    "### 4. Model Definition\n",
    "Defines the neural network model architecture for negation detection. The model combines word embeddings, POS tags, casing information, and character embeddings.\n",
    "\n",
    "### 5. Loss Function and Weights\n",
    "Prepares the loss function (`CrossEntropyLoss`) with custom weights to handle class imbalances. The weights are computed based on the frequency of each class in the training data.\n",
    "\n",
    "### 6. Model Initialization and Training\n",
    "Initializes the model, loss function, and optimizer. The model is then trained on the training data with early stopping based on validation loss.\n",
    "\n",
    "### 7. Saving the Model\n",
    "Saves the best model based on validation performance during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "import json \n",
    "from torch.utils.data import DataLoader, random_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_json(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    return data\n",
    "\n",
    "def save_json(data, file_path):\n",
    "    with open(file_path, 'w') as f:\n",
    "        json.dump(data, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format used: [[[sent1 w1 feature],[sent1 w2 feature],[sent1 w3 feature]],[[sent2 w1 feature],[sent2 w2 feature],[sent2 w3 feature]]...]\n",
    "# Each feature is depending the case embedding of len == 100 or a one hot vector. \n",
    "# Load data pre-processed.\n",
    "\n",
    "train_word_embedding_reduced = load_json('train_word_embeddings_reduced.json')\n",
    "train_casing_onehot =  load_json('train_casing_onehot.json')\n",
    "train_pos_onehot = load_json('train_pos_onehot.json')\n",
    "y_train = load_json('y_train_numerical.json')\n",
    "w_fast_vectors_reduced = load_json('w_fast_vectors_reduced.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, w_fast, w_vector, pos, casing, labels):\n",
    "        self.w_fast = torch.tensor(w_fast, dtype=torch.float32)\n",
    "        self.w_vector = torch.tensor(w_vector, dtype=torch.float32)\n",
    "        self.pos = torch.tensor(pos, dtype=torch.float32)\n",
    "        self.casing = torch.tensor(casing, dtype=torch.float32)\n",
    "        self.labels = torch.tensor(labels, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.w_fast)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (self.w_fast[idx], self.w_vector[idx], self.pos[idx], self.casing[idx]), self.labels[idx]\n",
    "\n",
    "\n",
    "# Split the dataset into training and validation sets\n",
    "dataset = CustomDataset(w_fast_vectors_reduced, train_word_embedding_reduced, train_pos_onehot, train_casing_onehot, y_train)\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataLoaders for training and validation sets\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# My gpu crashed everytime I try to use it. \n",
    "device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_index_tag={\"O\": 0, \"NEG\": 1, \"NSCO\": 2, \"UNC\": 3, \"USCO\": 4, \"PAD\":5}\n",
    "dict_index_tag_inverted={0:\"O\", 1:\"NEG\", 2:\"NSCO\", 3:\"UNC\", 4:\"USCO\", 5:\"PAD\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import numpy as np\n",
    "\n",
    "class NegationModel(nn.Module):\n",
    "    def __init__(self, word_embedding_dim, pos_embedding_dim, char_embedding_dim, casing_dim, lstm_dim, num_filters, kernel_size, conv_dropout_rate=0.5, lstm_dropout_rate=0.5, lstm_recurrent_dropout_rate=0.25):\n",
    "        super(NegationModel, self).__init__()\n",
    "        self.word_embedding_dim = word_embedding_dim\n",
    "        self.pos_embedding_dim = pos_embedding_dim\n",
    "        self.char_embedding_dim = char_embedding_dim\n",
    "        self.casing_dim = casing_dim\n",
    "        \n",
    "        self.conv1d = nn.Conv1d(in_channels=char_embedding_dim, out_channels=num_filters, kernel_size=kernel_size)\n",
    "        self.conv_dropout = nn.Dropout(conv_dropout_rate)\n",
    "        self.max_pool = nn.MaxPool1d(kernel_size=3)\n",
    "        \n",
    "        self.bi_lstm = nn.LSTM(425,\n",
    "                               hidden_size=lstm_dim, batch_first=True, bidirectional=True, num_layers=3,\n",
    "                               dropout=lstm_recurrent_dropout_rate)\n",
    "        \n",
    "        self.lstm_dropout = nn.Dropout(lstm_dropout_rate)\n",
    "        self.fc = nn.Linear(2 * lstm_dim, 6)\n",
    "\n",
    "        self.fc_pos = nn.Linear(pos_embedding_dim, pos_embedding_dim)\n",
    "        self.fc_casing = nn.Linear(casing_dim, casing_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        char_embeddings, word_embeddings, pos_embeddings, casing_info = x\n",
    "        \n",
    "        char_embeddings = char_embeddings.permute(0, 2, 1)\n",
    "        char_features = self.conv1d(char_embeddings)\n",
    "        char_features = self.conv_dropout(char_features)  # Dropout after Conv1D\n",
    "        char_features = self.max_pool(char_features)\n",
    "        char_features = char_features.view(char_features.size(0), -1, char_features.size(2))\n",
    "        \n",
    "        pos_embeddings = self.fc_pos(pos_embeddings)\n",
    "        casing_info = self.fc_casing(casing_info)\n",
    "\n",
    "        combined_features = torch.cat((word_embeddings, pos_embeddings, char_features, casing_info), dim=2)\n",
    "        \n",
    "        lstm_out, _ = self.bi_lstm(combined_features)\n",
    "        lstm_out = self.lstm_dropout(lstm_out)  # Dropout after LSTM output\n",
    "        \n",
    "        output = self.fc(lstm_out)\n",
    "        \n",
    "        return output\n",
    "\n",
    "def train(model, train_loader, val_loader, criterion, optimizer, num_epochs, patience=1):\n",
    "    model.train()\n",
    "    best_loss = float('inf')\n",
    "    epochs_no_improve = 0\n",
    "    early_stop = False\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        if early_stop:\n",
    "            print(\"Early stopping\")\n",
    "            break\n",
    "\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct_predictions = {i: 0 for i in range(6)}\n",
    "        total_predictions = {i: 0 for i in range(6)}\n",
    "\n",
    "        for idx, (inputs, labels) in enumerate(train_loader):\n",
    "            inputs = [inp.to(device) for inp in inputs]\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            # Reshape outputs and labels for CrossEntropyLoss\n",
    "            outputs = outputs.view(-1, outputs.size(-1))  # (batch_size * sequence_length, num_classes)\n",
    "            labels = labels.view(-1, labels.size(-1))  # (batch_size * sequence_length, num_classes)\n",
    "\n",
    "            # Convert one-hot labels to class indices\n",
    "            labels = torch.argmax(labels, dim=1)\n",
    "\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            # Calculate accuracy\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            for label in range(6):\n",
    "                correct_predictions[label] += ((predicted == labels) & (labels == label)).sum().item()\n",
    "                total_predictions[label] += (labels == label).sum().item()\n",
    "\n",
    "            if idx % 25 == 0:\n",
    "                print(f\"Batch = {idx} Running loss= {running_loss}\")\n",
    "\n",
    "        # Calculate accuracy for each class\n",
    "        accuracies = {label: (correct_predictions[label] / total_predictions[label] if total_predictions[label] > 0 else 0.0) for label in range(6)}\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {running_loss/len(train_loader)}\")\n",
    "        for label, accuracy in accuracies.items():\n",
    "            print(f\"Accuracy for class {label}: {accuracy * 100:.2f}%\")\n",
    "\n",
    "        # Validate the model\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for val_inputs, val_labels in val_loader:\n",
    "                val_inputs = [inp.to(device) for inp in val_inputs]\n",
    "                val_labels = val_labels.to(device)\n",
    "\n",
    "                val_outputs = model(val_inputs)\n",
    "                val_outputs = val_outputs.view(-1, val_outputs.size(-1))\n",
    "                val_labels = val_labels.view(-1, val_labels.size(-1))\n",
    "                val_labels = torch.argmax(val_labels, dim=1)\n",
    "\n",
    "                val_loss += criterion(val_outputs, val_labels).item()\n",
    "\n",
    "        val_loss /= len(val_loader)\n",
    "        print(f\"Validation Loss: {val_loss}\")\n",
    "\n",
    "        # Early stopping\n",
    "        if val_loss < best_loss:\n",
    "            best_loss = val_loss\n",
    "            epochs_no_improve = 0\n",
    "            # Save the model\n",
    "            save_path = 'top_model.pth'\n",
    "            torch.save(model.state_dict(), save_path)\n",
    "            print(f\"Model saved to {save_path}\")\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            if epochs_no_improve >= patience:\n",
    "                print(f\"Early stopping after {patience} epochs with no improvement\")\n",
    "                early_stop = True\n",
    "\"\"\" \n",
    "Labels shape: torch.Size([377, 6])\n",
    "Character embedding_dim: torch.Size([377, 100])\n",
    "Word embedding_dim: torch.Size([377, 100])\n",
    "OnehotPOS_dim: torch.Size([377, 17])\n",
    "OnehotCASING_dim: torch.Size([377, 8])\n",
    "\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_index_tag={\"O\": 0, \"NEG\": 1, \"NSCO\": 2, \"UNC\": 3, \"USCO\": 4, \"PAD\":5}\n",
    "dict_index_tag_inverted={0:\"O\", 1:\"NEG\", 2:\"NSCO\", 3:\"UNC\", 4:\"USCO\", 5:\"PAD\"}\n",
    "\n",
    "# Preparación de los pesos\n",
    "frecuencias = np.zeros(len(dict_index_tag))\n",
    "for sentence in y_train:\n",
    "    for word in sentence:\n",
    "        frecuencias += np.array(word)\n",
    "\n",
    "\n",
    "pesos = 1.0 / np.sqrt(frecuencias + 1e-8)\n",
    "pesos[dict_index_tag[\"PAD\"]] = 0.0  # El peso de \"PAD\" debe ser 0\n",
    "\n",
    "\n",
    "pesos = pesos / pesos.sum()\n",
    "\n",
    "\n",
    "pesos_tensor = torch.tensor(pesos, dtype=torch.float32).to(device)\n",
    "print(pesos_tensor)\n",
    "\n",
    "\n",
    "for label, weight in zip(dict_index_tag_inverted.values(), pesos):\n",
    "    print(f\"Peso para la etiqueta {label}: {weight:.4f}\")\n",
    "\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss(weight=pesos_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializar el modelo y transferirlo al dispositivo\n",
    "model = NegationModel(word_embedding_dim=100, pos_embedding_dim=17, char_embedding_dim=100, casing_dim=8, lstm_dim=150, num_filters=377, kernel_size=3)\n",
    "model.to(device)\n",
    "\n",
    "# Inicializar el criterio de pérdida y transferir los pesos al dispositivo\n",
    "criterion = nn.CrossEntropyLoss(weight=pesos_tensor.to(device))\n",
    "\n",
    "# Inicializar el optimizador\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "\n",
    "# Entrenar el modelo\n",
    "train(model, train_loader,val_loader, criterion, optimizer, num_epochs=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs, labels = train_dataset[0]\n",
    "shapes = [input.shape for input in inputs]\n",
    "\n",
    "print(f\"Labels shape: {labels.shape}\")\n",
    "print(f\"Character embedding_dim: {shapes[0]}\")\n",
    "print(f\"Word embedding_dim: {shapes[1]}\")\n",
    "print(f\"OnehotPOS_dim: {shapes[2]}\")\n",
    "print(f\"OnehotCASING_dim: {shapes[3]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NaturalLanguageProcessing",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
