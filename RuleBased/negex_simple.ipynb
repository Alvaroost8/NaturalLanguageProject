{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import nltk\n",
    "from spacy import detect \n",
    "import re \n",
    "import spacy\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f=open(\"training.json\")\n",
    "training_set=json.load(f)\n",
    "\n",
    "f2 = open(\"test.json\")\n",
    "test_set = json.load(f2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_training=[]\n",
    "for i in range(len(training_set)):\n",
    "    l_training.append({\"text\":training_set[i][\"data\"][\"text\"], \"results\":training_set[i][\"predictions\"][0][\"result\"]})\n",
    "\n",
    "l_test = []\n",
    "for i in range(len(test_set)):\n",
    "    l_test.append({\"text\":test_set[i][\"data\"][\"text\"], \"results\":test_set[i][\"predictions\"][0][\"result\"]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_neg=[]\n",
    "l_unc=[]\n",
    "l_nsco=[]\n",
    "l_usco=[]\n",
    "for dic in l_training:\n",
    "    for dic2 in dic[\"results\"]:\n",
    "        if(dic2[\"value\"][\"labels\"][0]==\"NEG\"):\n",
    "            l_neg.append(dic[\"text\"][dic2[\"value\"][\"start\"]:dic2[\"value\"][\"end\"]])\n",
    "        \n",
    "        elif(dic2[\"value\"][\"labels\"][0]==\"UNC\"):\n",
    "            l_unc.append(dic[\"text\"][dic2[\"value\"][\"start\"]:dic2[\"value\"][\"end\"]])\n",
    "        \n",
    "        elif(dic2[\"value\"][\"labels\"][0]==\"NSCO\"):\n",
    "            l_nsco.append(dic[\"text\"][dic2[\"value\"][\"start\"]:dic2[\"value\"][\"end\"]])\n",
    "        \n",
    "        elif(dic2[\"value\"][\"labels\"][0]==\"USCO\"):\n",
    "            l_usco.append(dic[\"text\"][dic2[\"value\"][\"start\"]:dic2[\"value\"][\"end\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(l_neg)):\n",
    "    if(not l_neg[i][0].isalpha()):\n",
    "        l_neg[i]=l_neg[i][1:]\n",
    "    if(not l_neg[i][-1].isalpha()):\n",
    "        l_neg[i]=l_neg[i][:-1]\n",
    "\n",
    "for i in range(len(l_unc)):\n",
    "    if(not l_unc[i][0].isalpha()):\n",
    "        l_unc[i]=l_unc[i][1:]\n",
    "    if(not l_unc[i][-1].isalpha()):\n",
    "        l_unc[i]=l_unc[i][:-1]\n",
    "\n",
    "for i in range(len(l_nsco)):\n",
    "    if(not l_nsco[i][0].isalpha()):\n",
    "        l_nsco[i]=l_nsco[i][1:]\n",
    "    if(not l_nsco[i][-1].isalpha()):\n",
    "        l_nsco[i]=l_nsco[i][:-1]\n",
    "\n",
    "for i in range(len(l_usco)):\n",
    "    if(not l_usco[i][0].isalpha()):\n",
    "        l_usco[i]=l_usco[i][1:]\n",
    "    if(not l_usco[i][-1].isalpha()):\n",
    "        l_usco[i]=l_usco[i][:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_neg=list(set(l_neg))\n",
    "l_unc=list(set(l_unc))\n",
    "l_nsco=list(set(l_nsco))\n",
    "l_usco=list(set(l_usco))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_words(list_words, text):\n",
    "    pattern = '|'.join(re.escape(palabra) for palabra in list_words)\n",
    "    pattern= r\"\\b(\" +pattern+ r\")\\b\"\n",
    "    \n",
    "    coincidencias = []\n",
    "    ini = []\n",
    "    end = []\n",
    "    \n",
    "    # Finding all occurrences of any word in the text\n",
    "    for match in re.finditer(pattern, text):\n",
    "        coincidencias.append(match.group(0))\n",
    "        ini.append(match.start())\n",
    "        end.append(match.end())    \n",
    "    return coincidencias, end, ini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"es_core_news_md\")\n",
    "\n",
    "# Function to extract medical terms.\n",
    "def extract_medical_terms(text):\n",
    "    doc = nlp(text)\n",
    "    medical_terms = []\n",
    "    for token in doc:\n",
    "        if token.pos_ == \"NOUN\" or token.pos_ == \"ADJ\":\n",
    "            medical_terms.append(token.text)\n",
    "    return medical_terms\n",
    "\n",
    "l_medical_term2 = []\n",
    "\n",
    "for sentence in l_nsco:\n",
    "    medical_terms = extract_medical_terms(sentence)\n",
    "    l_medical_term2.append(medical_terms)\n",
    "\n",
    "for sentence in l_usco:\n",
    "    medical_terms = extract_medical_terms(sentence)\n",
    "    l_medical_term2.append(medical_terms)\n",
    "\n",
    "l_medical_terms =[]\n",
    "for x in range (len(l_medical_term2)):\n",
    "    for y in range(len(l_medical_term2[x])):\n",
    "        l_medical_terms.append(l_medical_term2[x][y])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_medical_terms = list(set(l_medical_terms))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eliminate_patient_information(string):\n",
    "    index = string.find(\"informe\")\n",
    "    if index != -1:\n",
    "        return string[index:]\n",
    "    else:\n",
    "        return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auxiliary Function\n",
    "def find_scope_baseline(target_word, sentence, l_medical):\n",
    "    \"\"\"\n",
    "    \n",
    "    It returns False in case there is not a medical term near.\n",
    "\n",
    "    Otherwise it returns the ini and end indexes of the scope.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    language = detect(sentence)\n",
    "    if language == 'ca': \n",
    "        nlp = spacy.load(\"ca_core_news_sm\")\n",
    "    elif language == 'es':\n",
    "        nlp = spacy.load(\"es_core_news_sm\")\n",
    "    else:\n",
    "        nlp = spacy.load(\"es_core_news_sm\")\n",
    "\n",
    "    doc = nlp(sentence)\n",
    "\n",
    "    keyword_token = None\n",
    "    for token in doc:\n",
    "        if token.text.lower() == target_word.lower():\n",
    "            keyword_token = token\n",
    "            break\n",
    "    \n",
    "    if not keyword_token:\n",
    "        return False, False\n",
    "\n",
    "    nearest_medical_term = None\n",
    "    nearest_distance = float('inf')\n",
    "\n",
    "\n",
    "    for token in doc:\n",
    "        if token.text.lower() in l_medical:         \n",
    "            distance = abs(token.i - keyword_token.i)\n",
    "            if distance < nearest_distance:\n",
    "                nearest_medical_term = token.text\n",
    "                nearest_distance = distance\n",
    "\n",
    "\n",
    "    keyword_index = keyword_token.i \n",
    "    \n",
    "    if nearest_medical_term == None:\n",
    "        return False,False\n",
    "    medical_index = [token.i for token in doc if token.text.lower() == nearest_medical_term.lower()][0]   \n",
    "    \n",
    "    if keyword_index < medical_index:\n",
    "        ini_index = keyword_index + 1\n",
    "        end_index = medical_index \n",
    "    else:\n",
    "        ini_index = medical_index \n",
    "        end_index = keyword_index -1\n",
    "\n",
    "\n",
    "    # Obtener el texto del \"scope\"\n",
    "    scope_text = doc[ini_index:end_index + 1].text\n",
    "\n",
    "    inicio = sentence.find(scope_text)\n",
    "    fin = inicio + len(scope_text)\n",
    "\n",
    "    return inicio, fin\n",
    "\n",
    "target_word= 'no'\n",
    "sentence = \"El personaje no tiene madera de \"\n",
    "l_medical = ['paciente']\n",
    "a,b = find_scope_baseline(target_word, sentence, l_medical)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auxiliary Function\n",
    "def find_scope_baseline(target_word, sentence, l_medical):\n",
    "    \"\"\"\n",
    "    \n",
    "    It returns False in case there is not a medical term near.\n",
    "\n",
    "    Otherwise it returns the ini and end indexes of the scope.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    language = detect(sentence)\n",
    "    if language == 'ca': \n",
    "        nlp = spacy.load(\"ca_core_news_sm\")\n",
    "    elif language == 'es':\n",
    "        nlp = spacy.load(\"es_core_news_sm\")\n",
    "    else:\n",
    "        nlp = spacy.load(\"es_core_news_sm\")\n",
    "\n",
    "    doc = nlp(sentence)\n",
    "\n",
    "    keyword_token = None\n",
    "    for token in doc:\n",
    "        if token.text.lower() == target_word.lower():\n",
    "            keyword_token = token\n",
    "            break\n",
    "    \n",
    "    if not keyword_token:\n",
    "        return False, False\n",
    "\n",
    "    nearest_medical_term = None\n",
    "    nearest_distance = float('inf')\n",
    "\n",
    "\n",
    "    for token in doc:\n",
    "        if token.text.lower() in l_medical:         \n",
    "            distance = abs(token.i - keyword_token.i)\n",
    "            if distance < nearest_distance:\n",
    "                nearest_medical_term = token.text\n",
    "                nearest_distance = distance\n",
    "\n",
    "\n",
    "    keyword_index = keyword_token.i \n",
    "    \n",
    "    if nearest_medical_term == None:\n",
    "        return False,False\n",
    "    medical_index = [token.i for token in doc if token.text.lower() == nearest_medical_term.lower()][0]   \n",
    "    \n",
    "    if keyword_index < medical_index:\n",
    "        ini_index = keyword_index + 1\n",
    "        end_index = medical_index \n",
    "    else:\n",
    "        ini_index = medical_index \n",
    "        end_index = keyword_index -1\n",
    "\n",
    "\n",
    "    # Obtener el texto del \"scope\"\n",
    "    scope_text = doc[ini_index:end_index + 1].text\n",
    "\n",
    "    inicio = sentence.find(scope_text)\n",
    "    fin = inicio + len(scope_text)\n",
    "\n",
    "    return inicio, fin\n",
    "\n",
    "target_word= 'no'\n",
    "sentence = \"El personaje no tiene madera de \"\n",
    "l_medical = ['paciente']\n",
    "a,b = find_scope_baseline(target_word, sentence, l_medical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "mini_prueba = l_training[:10]\n",
    "print(len(mini_prueba))\n",
    "\n",
    "l_res = [] # List to store the final results. \n",
    "\n",
    "keywords_voc = l_neg + l_unc\n",
    "for element in mini_prueba:\n",
    "    text = eliminate_patient_information(element['text'])\n",
    "    counter = len(element['text']) - len(text)\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    scopes = []\n",
    "    dictionary_res_list = []\n",
    "\n",
    "    for sentence in sentences:\n",
    "        keywords_appear, end_keyword, start_keyword = search_words(keywords_voc,sentence)\n",
    "\n",
    "        for i in range(len(keywords_appear)):\n",
    "            if keywords_appear[i] in l_neg:\n",
    "                label = [\"NEG\"]\n",
    "                res = {\"value\":{\"start\": start_keyword[i] + counter, \"end\": end_keyword[i] + counter, \"labels\": label}}\n",
    "                dictionary_res_list.append(res)\n",
    "\n",
    "                ini,end = find_scope_baseline(keywords_appear[i], sentence, l_medical_terms)\n",
    "                if ini != False:\n",
    "                    label = [\"NSCO\"]\n",
    "                    res = {\"value\":{\"start\": ini + counter, \"end\": end + counter, \"labels\": label}}\n",
    "                    dictionary_res_list.append(res)\n",
    "\n",
    "            else:\n",
    "\n",
    "                label = [\"UNC\"]\n",
    "                ini,end = find_scope_baseline(keywords_appear[i], sentence, l_medical_terms)\n",
    "                res = {\"value\":{\"start\": start_keyword[i] + counter, \"end\": end_keyword[i] + counter, \"labels\": label}}\n",
    "                dictionary_res_list.append(res)\n",
    "\n",
    "                ini,end = find_scope_baseline(keywords_appear[i], sentence, l_medical_terms)\n",
    "                if ini != False:\n",
    "                    label = [\"USCO\"]\n",
    "                    res = {\"value\":{\"start\": ini + counter, \"end\": end + counter, \"labels\": label}}\n",
    "                    dictionary_res_list.append(res)\n",
    "\n",
    "        counter+=len(sentence)\n",
    "                    \n",
    "\n",
    "    l_res.append({'text':element['text'], 'result': dictionary_res_list})\n",
    "        \n",
    "print(l_res)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the resulting json\n",
    "with open(\"res_training_10_Simplification.json\", \"w\") as archivo:\n",
    "    json.dump(l_res, archivo)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
