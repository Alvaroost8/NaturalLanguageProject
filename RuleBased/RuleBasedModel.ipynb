{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Project (Rule Based)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Álvaro Sáenz-Torre, Alejandra Reinares, Luis Domene and Joan Bayona"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import json\n",
    "import spacy_udpipe\n",
    "from langdetect import detect\n",
    "import spacy_udpipe\n",
    "from spacy import displacy\n",
    "import nltk\n",
    "from nltk.corpus import cess_esp, cess_cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package cess_esp to\n",
      "[nltk_data]     C:\\Users\\alvar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package cess_esp is already up-to-date!\n",
      "[nltk_data] Downloading package cess_cat to\n",
      "[nltk_data]     C:\\Users\\alvar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package cess_cat is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\alvar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('cess_esp')\n",
    "nltk.download('cess_cat')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "f=open(\"training.json\")\n",
    "training_set=json.load(f)\n",
    "\n",
    "f2 = open(\"test.json\")\n",
    "test_set = json.load(f2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_training=[]\n",
    "for i in range(len(training_set)):\n",
    "    l_training.append({\"text\":training_set[i][\"data\"][\"text\"], \"results\":training_set[i][\"predictions\"][0][\"result\"]})\n",
    "\n",
    "l_test = []\n",
    "for i in range(len(test_set)):\n",
    "    l_test.append({\"text\":test_set[i][\"data\"][\"text\"], \"results\":test_set[i][\"predictions\"][0][\"result\"]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_neg=[]\n",
    "l_unc=[]\n",
    "l_nsco=[]\n",
    "l_usco=[]\n",
    "for dic in l_training:\n",
    "    for dic2 in dic[\"results\"]:\n",
    "        if(dic2[\"value\"][\"labels\"][0]==\"NEG\"):\n",
    "            l_neg.append(dic[\"text\"][dic2[\"value\"][\"start\"]:dic2[\"value\"][\"end\"]])\n",
    "        \n",
    "        elif(dic2[\"value\"][\"labels\"][0]==\"UNC\"):\n",
    "            l_unc.append(dic[\"text\"][dic2[\"value\"][\"start\"]:dic2[\"value\"][\"end\"]])\n",
    "        \n",
    "        elif(dic2[\"value\"][\"labels\"][0]==\"NSCO\"):\n",
    "            l_nsco.append(dic[\"text\"][dic2[\"value\"][\"start\"]:dic2[\"value\"][\"end\"]])\n",
    "        \n",
    "        elif(dic2[\"value\"][\"labels\"][0]==\"USCO\"):\n",
    "            l_usco.append(dic[\"text\"][dic2[\"value\"][\"start\"]:dic2[\"value\"][\"end\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(l_neg)):\n",
    "    if(not l_neg[i][0].isalpha()):\n",
    "        l_neg[i]=l_neg[i][1:]\n",
    "    if(not l_neg[i][-1].isalpha()):\n",
    "        l_neg[i]=l_neg[i][:-1]\n",
    "\n",
    "for i in range(len(l_unc)):\n",
    "    if(not l_unc[i][0].isalpha()):\n",
    "        l_unc[i]=l_unc[i][1:]\n",
    "    if(not l_unc[i][-1].isalpha()):\n",
    "        l_unc[i]=l_unc[i][:-1]\n",
    "\n",
    "for i in range(len(l_nsco)):\n",
    "    if(not l_nsco[i][0].isalpha()):\n",
    "        l_nsco[i]=l_nsco[i][1:]\n",
    "    if(not l_nsco[i][-1].isalpha()):\n",
    "        l_nsco[i]=l_nsco[i][:-1]\n",
    "\n",
    "for i in range(len(l_usco)):\n",
    "    if(not l_usco[i][0].isalpha()):\n",
    "        l_usco[i]=l_usco[i][1:]\n",
    "    if(not l_usco[i][-1].isalpha()):\n",
    "        l_usco[i]=l_usco[i][:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_neg=list(set(l_neg))\n",
    "l_unc=list(set(l_unc))\n",
    "l_nsco=list(set(l_nsco))\n",
    "l_usco=list(set(l_usco))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the list of conjunctions\n",
    "t=open(\"conjunctions.txt\")\n",
    "conjunctions=t.read()\n",
    "\n",
    "l_conj=[]\n",
    "s=\"\"\n",
    "for l in conjunctions:\n",
    "    if(l!=\"\\n\"):\n",
    "        s+=l\n",
    "    else:\n",
    "        l_conj.append(s)\n",
    "        s=\"\"\n",
    "\n",
    "l_conj=list(set(l_conj[3:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_words(list_words, text):\n",
    "    pattern = '|'.join(re.escape(palabra) for palabra in list_words)\n",
    "    pattern= r\"\\b(\" +pattern+ r\")\\b\"\n",
    "    \n",
    "    coincidencias = []\n",
    "    ini = []\n",
    "    end = []\n",
    "    \n",
    "    # Finding all occurrences of any word in the text\n",
    "    for match in re.finditer(pattern, text):\n",
    "        coincidencias.append(match.group(0))\n",
    "        ini.append(match.start())\n",
    "        end.append(match.end())    \n",
    "    return coincidencias, end, ini"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rule 1: if the sentence contains a termination term, the scope is extracted using this term. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rule1(start_keyword, end_keyword, text, l_term):\n",
    "    \"\"\"\n",
    "    Parameters: {\n",
    "    start_keyword: list of starting points.\n",
    "    end_keyword: list of ending points.\n",
    "    text: sentence\n",
    "    l_term:  list of termination terms.\n",
    "    }\n",
    "\n",
    "    Output{\n",
    "    l_pos_scope: list of scopes [[12,32],[40,78]]\n",
    "    }\n",
    "    \n",
    "    \"\"\"\n",
    "    l_pos_scope=[]\n",
    "    for i in range(len(end_keyword)):\n",
    "        idx=end_keyword[i]\n",
    "        found=False\n",
    "        if(i==len(end_keyword)-1):\n",
    "            while not found and idx<len(text):\n",
    "                if(text[idx]==\" \"):\n",
    "                    idx+=1\n",
    "                    for term in l_term:\n",
    "                        if((idx+len(term))<len(text) and term==text[idx:idx+len(term)]):\n",
    "                            found=True\n",
    "                            l_pos_scope.append([end_keyword[i]+1,idx-1])\n",
    "                else:\n",
    "                    idx+=1\n",
    "        else:\n",
    "            while not found and idx<start_keyword[i+1]:\n",
    "                if(text[idx]==\" \"):\n",
    "                    idx+=1\n",
    "                    for term in l_term:\n",
    "                        if((idx+len(term))<len(text) and term==text[idx:idx+len(term)]):\n",
    "                            found=True\n",
    "                            l_pos_scope.append([end_keyword[i]+1,idx-1])\n",
    "                else:\n",
    "                    idx+=1\n",
    "        if not found:\n",
    "            l_pos_scope.append([])\n",
    "    return l_pos_scope"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rule 2: If a cue is detected in a sentence containing contiguous cues the scope will be given by the position Ci+1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rule2(keywords,ini_keyword,final_keyword,sentence):\n",
    "    \"\"\"\n",
    "    Parameters{\n",
    "    keywords: list of keywords (negations and uncertainty)\n",
    "    final_keyword: list of endpoints\n",
    "    sentence: sentence where you're applying rule 2. \n",
    "    }\n",
    "\n",
    "    Output{\n",
    "    l_scopes: list of scopes [[12,32],[40,78]]\n",
    "    }\n",
    "    \n",
    "       \"\"\"\n",
    "    l_scopes = []\n",
    "\n",
    "    #sentences = nltk.sent_tokenize(text)\n",
    "    if (len(keywords)== 1):\n",
    "        return [[]]\n",
    "    \n",
    "    if(len(keywords)>0):\n",
    "        while len(keywords) > 1:\n",
    "            if ',' in sentence[ini_keyword[1]-4:ini_keyword[1]]:\n",
    "                l_scopes.append([final_keyword[0],ini_keyword[1]])\n",
    "          \n",
    "            else:\n",
    "                l_scopes.append([])\n",
    "\n",
    "            keywords.pop(0)\n",
    "            ini_keyword.pop(0)\n",
    "            final_keyword.pop(0)\n",
    "    \n",
    "    l_scopes.append([])\n",
    "\n",
    "    return l_scopes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rule3(keywords,final_keyword,sentence):\n",
    "\n",
    "    \"\"\"\n",
    "    Parameters{\n",
    "    keywords: list of keywords (negations and uncertainty)\n",
    "    sentence: sentence where you're applying rule 3. \n",
    "    final_keyword: list of end points.\n",
    "    }\n",
    "\n",
    "    Output{\n",
    "    l_scopes: list of scopes [[12,32],[40,78]]\n",
    "    }\n",
    "    \n",
    "    \"\"\"\n",
    "    scopes = []\n",
    "    if keywords:\n",
    "        for i in range(len(keywords)):\n",
    "            possible_scope = str(sentence[final_keyword[i]:]).split()[:3]\n",
    "            dot = False\n",
    "            for j in range(len(possible_scope)):\n",
    "                if \".\" in possible_scope[j]: \n",
    "                    dot = True\n",
    "                    break\n",
    "\n",
    "            if dot:   \n",
    "                dot_search = sentence[final_keyword[i]:]   \n",
    "                for r in range(len(dot_search)):\n",
    "                    if dot_search[r] == \".\":\n",
    "                            end = r + final_keyword[i]\n",
    "                            break\n",
    "                scopes.append([final_keyword[i], end])\n",
    "            else:\n",
    "                scopes.append([])     \n",
    "    return scopes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rule 4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_esp = cess_esp.tagged_sents()\n",
    "corpus_cat = cess_cat.tagged_sents()\n",
    "\n",
    "tnt_tagger = nltk.tag.tnt.TnT()\n",
    "tnt_tagger.train(corpus_esp)\n",
    "tnt_tagger.train(corpus_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rule4(start_keyword, end_keyword, text, tnt_tagger):\n",
    "  \"\"\"\n",
    "    Parameters: {\n",
    "     start_keyword: list of starting points.\n",
    "    end_keyword: list of ending points.\n",
    "    text: sentence\n",
    "    tnt_tagger:  model trained in catalan/spanish to POS tag.\n",
    "    }\n",
    "\n",
    "    Output{\n",
    "    l_pos_scope: list of scopes [[12,32],[40,78]]\n",
    "    }\n",
    "    \n",
    "  \"\"\"\n",
    "  l_pos_scope=[]\n",
    "  tokens=word_tokenize(text)\n",
    "  for i in tokens:\n",
    "    if(not i.isalpha()):\n",
    "      tokens.remove(i)\n",
    "  tags = tnt_tagger.tag(tokens)\n",
    "  l_text=text.split(\" \")\n",
    "  for i in l_text:\n",
    "    if(not i.isalpha()):\n",
    "      l_text.remove(i)\n",
    "  l_pos=[0,]\n",
    "  p=0\n",
    "  for i in range(len(l_text)-1):\n",
    "    p+=(len(l_text[i])+1)\n",
    "    l_pos.append(p)\n",
    "  for i in range(len(end_keyword)):\n",
    "    idx=end_keyword[i]\n",
    "    found=False\n",
    "    x=False\n",
    "    if(i==len(end_keyword)-1):\n",
    "      while not found and idx<len(text):\n",
    "        if(idx in l_pos):\n",
    "          if x:\n",
    "            index=l_pos.index(idx)\n",
    "            if(tags[index][1][0]==\"c\"):\n",
    "              found=True\n",
    "              l_pos_scope.append((end_keyword[i]+1,idx-1))\n",
    "            elif(tags[index][1][0]==\"v\" and tags[index][1][:3]!=\"vmp\" and tags[index][1][:3]!=\"vmg\"):\n",
    "              found=True\n",
    "              l_pos_scope.append((end_keyword[i]+1,idx-1))\n",
    "          else:\n",
    "            x=True\n",
    "        idx+=1\n",
    "    else:\n",
    "      while not found and idx<start_keyword[i+1]:\n",
    "        if(idx in l_pos):\n",
    "          if x:\n",
    "            index=l_pos.index(idx)\n",
    "            if(tags[index][1][0]==\"c\"):\n",
    "              found=True\n",
    "              l_pos_scope.append((end_keyword[i]+1,idx-1))\n",
    "            elif(tags[index][1][0]==\"v\" and tags[index][1][:3]!=\"vmp\" and tags[index][1][:3]!=\"vmg\"):\n",
    "              found=True\n",
    "              l_pos_scope.append((end_keyword[i]+1,idx-1))\n",
    "          else:\n",
    "            x=True\n",
    "        idx+=1\n",
    "    if not found:\n",
    "      l_pos_scope.append([])\n",
    "  return l_pos_scope"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rule 5: if the sentence does not match the previous rules, the algorithm generate a sentence parse tree. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auxiliar Function Rule 5\n",
    "def find_scope(sentence,word,nlp):\n",
    "    \"\"\"\n",
    "    Parameters: {    \n",
    "    sentence: Sentence to be parsed searching the scope\n",
    "    word: That you are interested in finding the scope\n",
    "    nlp: Object that is used to extract the sentence information it can be either spanish_nlp or catalan_nlp\n",
    "    }\n",
    "    \n",
    "    Output{\n",
    "    scope : example [12,39]\n",
    "    }\n",
    "    \"\"\"\n",
    "    doc = nlp(sentence)\n",
    "    \n",
    "    root = [token for token in doc if token.dep_ == \"ROOT\"][0]\n",
    "    childs = [child for child in root.children]\n",
    "    start =  4\n",
    "    scope = []\n",
    "    for child in childs: # Sugestivos -> nódulos -> fiebre\n",
    "      if child.text == word:\n",
    "        start = 0\n",
    "      else:\n",
    "        start += 1\n",
    "\n",
    "      if start == 1:\n",
    "          grandchildren = [grandchild.text for grandchild in child.children]\n",
    "          for grandchild in child.children:\n",
    "            greatgrandchildren = [greatgrandchild.text for greatgrandchild in grandchild.children]\n",
    "        \n",
    "            scope = [child.text] + grandchildren + greatgrandchildren  \n",
    "    \n",
    "    start =  4\n",
    "    grandchildren_list = []\n",
    "    for child in childs: \n",
    "      for grandchildren in child.children:\n",
    "\n",
    "        if grandchildren.text == word:\n",
    "          start = 1\n",
    "          head = [grandchildren.head.text]\n",
    "\n",
    "        elif start == 1: # It needs to enter here every time after grandchildren.text == word\n",
    "            grandchildren_list.append(grandchildren.text)\n",
    "            greatgrandchildren_list = [greatgrandchildren.text for greatgrandchildren in grandchildren.children]\n",
    "\n",
    "            for greatgrandchildren in grandchildren.children:\n",
    "              greatgreatgrandchildren_list = [greatgreatgrandchildren.text for greatgreatgrandchildren in greatgrandchildren.children ]\n",
    "\n",
    "              scope = head + grandchildren_list + greatgrandchildren_list + greatgreatgrandchildren_list\n",
    "\n",
    "        \n",
    "    start_index = sentence.lower().find(word) + len(word)\n",
    "    old_index = 1000\n",
    "    end_index = -1\n",
    "    distance = start_index\n",
    "\n",
    "    for word in scope: \n",
    "\n",
    "        begin_index = sentence.lower().find(word)\n",
    "\n",
    "        if begin_index > start_index and begin_index < old_index:\n",
    "            old_index = begin_index\n",
    "\n",
    "        after_cue = sentence[start_index:].split()[:len(scope)] \n",
    "\n",
    "        for i in range(len(after_cue)):\n",
    "           distance += len(after_cue[i])\n",
    " \n",
    "        last_index = sentence.lower().find(word)\n",
    "       \n",
    "        if last_index < distance and last_index > end_index:\n",
    "            end_index = last_index + len(word)\n",
    "\n",
    "    result = [old_index, end_index]\n",
    "    if old_index == 1000 and end_index==-1:\n",
    "      return []\n",
    "    \n",
    "    else: \n",
    "       return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rule5(keywords,sentence):\n",
    "    \"\"\"\n",
    "    Parameters{\n",
    "    keywords: list of keywords (negations and uncertainty)\n",
    "    sentence: sentence where you're applying rule 3. \n",
    "    }\n",
    "\n",
    "    Output{\n",
    "    l_scopes: list of scopes [[12,32],[40,78]]\n",
    "    }\n",
    "    \n",
    "    \"\"\"\n",
    "    l_scopes = []\n",
    "    language = detect(sentence)\n",
    "    if language == 'ca':  # Catalán\n",
    "        nlp_ca = spacy_udpipe.load(\"ca\")\n",
    "\n",
    "        for word in keywords:\n",
    "            l_scopes.append(find_scope(sentence,word,nlp_ca)) \n",
    "            \n",
    "\n",
    "    else:  # Castellano\n",
    "        nlp_es = spacy.load(\"es_core_news_md\")\n",
    "        for word in keywords:\n",
    "            l_scopes.append(find_scope(sentence,word,nlp_es))\n",
    "    \n",
    "    return l_scopes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para cortar el string a partir de la palabra \"informe\"\n",
    "def eliminate_patient_information(string):\n",
    "    index = string.find(\"informe\")\n",
    "    if index != -1:\n",
    "        return string[index:]\n",
    "    else:\n",
    "        return string\n",
    "\n",
    "def eliminate_final_information(string):\n",
    "    index = string.find(\"destinacio a l'alta\")\n",
    "    if index != -1:\n",
    "        return string[:index]\n",
    "    else:\n",
    "        return string\n",
    "\n",
    "def check_complete(list):\n",
    "    for sublist in list:\n",
    "        if len(sublist)<1:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "def update_lists(a, b):\n",
    "    c = []\n",
    "    for list_a, list_b in zip(a, b):\n",
    "        if not list_a and list_b:\n",
    "            c.append(list_b)\n",
    "        else:\n",
    "            c.append(list_a)\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(input, l_neg, l_unc):\n",
    "    l_res = [] # List to store the final results. \n",
    "\n",
    "    keywords_voc = l_neg + l_unc\n",
    "    a=0\n",
    "    for element in input:\n",
    "        a+=1\n",
    "        text = eliminate_patient_information(element['text'])\n",
    "        counter = len(element['text']) - len(text)\n",
    "        sentences = nltk.sent_tokenize(text)\n",
    "        scopes = []\n",
    "        dictionary_res_list = []\n",
    "\n",
    "        for sentence in sentences:\n",
    "            keywords_appear, end_keyword, start_keyword = search_words(keywords_voc,sentence)\n",
    "\n",
    "            scopes = rule1(start_keyword,end_keyword,sentence,l_conj)\n",
    "\n",
    "            if scopes:\n",
    "                if check_complete(scopes) == False:\n",
    "                    scopes2 = rule2(keywords_appear.copy(),start_keyword.copy(),end_keyword.copy(),sentence)\n",
    "                    new_scope = update_lists(scopes,scopes2)\n",
    "                    \n",
    "                    if check_complete(new_scope) == False:\n",
    "                        scopes3 = rule3(keywords_appear,end_keyword,sentence)\n",
    "                        new_scope = update_lists(new_scope,scopes3)\n",
    "\n",
    "                        if check_complete(new_scope)== False:\n",
    "                            scopes4 = rule4(start_keyword,end_keyword,sentence,tnt_tagger)\n",
    "                            new_scope= update_lists(new_scope,scopes4)\n",
    "\n",
    "                            if check_complete(new_scope)== False:\n",
    "                                scopes5 = rule5(keywords_appear,sentence)\n",
    "                                new_scope= update_lists(new_scope,scopes5)\n",
    "\n",
    "                else:\n",
    "                    new_scope = scopes\n",
    "\n",
    "            for i in range(len(keywords_appear)):\n",
    "                if keywords_appear[i] in l_neg:\n",
    "                    label = [\"NEG\"]\n",
    "                    res = {\"value\":{\"start\": start_keyword[i] + counter, \"end\": end_keyword[i] + counter, \"labels\": label}}\n",
    "                    dictionary_res_list.append(res)\n",
    "                        \n",
    "                    if new_scope[i] != []:\n",
    "                        label = [\"NSCO\"]\n",
    "                        res = {\"value\":{\"start\": new_scope[i][0]+ counter, \"end\": new_scope[i][1]+counter, \"labels\": label}}\n",
    "                        dictionary_res_list.append(res)\n",
    "                else:\n",
    "                    label = [\"UNC\"]\n",
    "                    res = {\"value\":{\"start\": start_keyword[i] + counter, \"end\": end_keyword[i]+ counter, \"labels\": label}}\n",
    "                    dictionary_res_list.append(res)\n",
    "                    if new_scope[i] != []:\n",
    "                        label = [\"USCO\"]\n",
    "                        res = {\"value\":{\"start\": new_scope[i][0]+counter, \"end\": new_scope[i][1] + counter, \"labels\": label}}\n",
    "                        dictionary_res_list.append(res)\n",
    "\n",
    "            counter+=len(sentence)\n",
    "\n",
    "        l_res.append({'text':element['text'], 'result': dictionary_res_list})\n",
    "    return l_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_res_training=model(l_training, l_neg, l_unc)\n",
    "l_res_test=model(l_test, l_neg, l_unc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"res_training_Rules.json\", \"w\") as f:\n",
    "    json.dump(l_res_training, f)\n",
    "\n",
    "with open(\"res_test_Rules.json\", \"w\") as f:\n",
    "    json.dump(l_res_test, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NaturalLanguageProcessing",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
